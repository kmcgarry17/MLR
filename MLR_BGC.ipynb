{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ncdump'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-7d2a6772247f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mncdump\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mncdump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ncdump'"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import pickle as pk\n",
    "import statistics as st\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from ncdump import ncdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['theta_s', 'theta_b', 'Tcline', 'hc', 's_rho', 's_w', 'Cs_r', 'Cs_w', 'h', 'ocean_time', 'zeta_south', 'zeta_east', 'temp_east', 'temp_south', 'salt_east', 'salt_south', 'angle', 'lat_psi', 'lat_rho', 'lat_u', 'lat_v', 'lon_psi', 'lon_rho', 'lon_u', 'lon_v', 'mask_psi', 'mask_rho', 'mask_u', 'mask_v', 'pm', 'pn', 'u_east', 'u_south', 'ubar_east', 'ubar_south', 'v_east', 'v_south', 'vbar_east', 'vbar_south'])\n"
     ]
    }
   ],
   "source": [
    "bcfile = \"/d1/enrique/NWA_bdry_SODA3.3.1_y2012.nc\"\n",
    "bcds = nc.Dataset(bcfile)\n",
    "print(bcds.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature \n",
    "x = loadmat('absT.mat')\n",
    "Te = x['Te']\n",
    "Ts = x['Ts']\n",
    "\n",
    "# salinity\n",
    "Ss = bcds.variables['salt_south'][:]\n",
    "Se = bcds.variables['salt_east'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['theta_s', 'theta_b', 'Tcline', 'hc', 's_rho', 's_w', 'Cs_r', 'Cs_w', 'h', 'ocean_time', 'alk_north', 'alk_south', 'alk_west', 'alk_east', 'cadet_arag_north', 'cadet_arag_south', 'cadet_arag_west', 'cadet_arag_east', 'cadet_calc_north', 'cadet_calc_south', 'cadet_calc_west', 'cadet_calc_east', 'dic_north', 'dic_south', 'dic_west', 'dic_east', 'ldon_north', 'ldon_south', 'ldon_west', 'ldon_east', 'lith_north', 'lith_south', 'lith_west', 'lith_east', 'lithdet_north', 'lithdet_south', 'lithdet_west', 'lithdet_east', 'nbact_north', 'nbact_south', 'nbact_west', 'nbact_east', 'ndet_north', 'ndet_south', 'ndet_west', 'ndet_east', 'ndi_north', 'ndi_south', 'ndi_west', 'ndi_east', 'nlg_north', 'nlg_south', 'nlg_west', 'nlg_east', 'nsm_north', 'nsm_south', 'nsm_west', 'nsm_east', 'nh4_north', 'nh4_south', 'nh4_west', 'nh4_east', 'no3_north', 'no3_south', 'no3_west', 'no3_east', 'po4_north', 'po4_south', 'po4_west', 'po4_east', 'o2_north', 'o2_south', 'o2_west', 'o2_east', 'srdon_north', 'srdon_south', 'srdon_west', 'srdon_east', 'sldon_north', 'sldon_south', 'sldon_west', 'sldon_east', 'sidet_north', 'sidet_south', 'sidet_west', 'sidet_east', 'silg_north', 'silg_south', 'silg_west', 'silg_east', 'sio4_north', 'sio4_south', 'sio4_west', 'sio4_east', 'nsmz_north', 'nsmz_south', 'nsmz_west', 'nsmz_east', 'nmdz_north', 'nmdz_south', 'nmdz_west', 'nmdz_east', 'nlgz_north', 'nlgz_south', 'nlgz_west', 'nlgz_east'])\n"
     ]
    }
   ],
   "source": [
    "biofile = \"/d1/enrique/NWA_bdry_bio.nc\"\n",
    "biods = nc.Dataset(biofile)\n",
    "print(biods.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2s = biods.variables['o2_south'][:]\n",
    "o2e = biods.variables['o2_east'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize predictor variables\n",
    "Tm = 13.1856\n",
    "Tsd = 5.1979\n",
    "\n",
    "Sm = 34.4037\n",
    "Ssd = 1.4914\n",
    "\n",
    "O2m = 214.6618\n",
    "O2sd = 44.6693\n",
    "\n",
    "Ten = (Te-Tm)/Tsd\n",
    "Sen = (Se-Sm)/Ssd\n",
    "\n",
    "Tsn = (Ts-Tm)/Tsd\n",
    "Ssn = (Ss-Sm)/Ssd\n",
    "\n",
    "O2sn = (o2s-O2m)/O2sd\n",
    "O2en = (o2e-O2m)/O2sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (73,40,362) (12,40,362) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a8fa40888368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDICe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2099\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m52.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTen\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m34.7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSen\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m29.7\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mO2en\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3.29\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTen\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/analysis/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delegate_binop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4085\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/analysis/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, a, b, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m         \u001b[0;31m# Get the mask for the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (73,40,362) (12,40,362) "
     ]
    }
   ],
   "source": [
    "DICe = 2099-52.4*Ten+34.7*Sen-29.7*O2en-3.29*Ten*Sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 40, 362)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 40, 362)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 40, 362)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O2en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('s_rho',\n",
       "              <class 'netCDF4._netCDF4.Dimension'>: name = 's_rho', size = 40),\n",
       "             ('s_w',\n",
       "              <class 'netCDF4._netCDF4.Dimension'>: name = 's_w', size = 41),\n",
       "             ('eta_rho',\n",
       "              <class 'netCDF4._netCDF4.Dimension'>: name = 'eta_rho', size = 362),\n",
       "             ('xi_rho',\n",
       "              <class 'netCDF4._netCDF4.Dimension'>: name = 'xi_rho', size = 722),\n",
       "             ('ocean_time',\n",
       "              <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'ocean_time', size = 12)])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biods.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package netCDF4:\n",
      "\n",
      "NAME\n",
      "    netCDF4\n",
      "\n",
      "DESCRIPTION\n",
      "    Version 1.5.1.2\n",
      "    ---------------\n",
      "    - - -\n",
      "    \n",
      "    Introduction\n",
      "    ============\n",
      "    \n",
      "    netcdf4-python is a Python interface to the netCDF C library.\n",
      "    \n",
      "    [netCDF](http://www.unidata.ucar.edu/software/netcdf/) version 4 has many features\n",
      "    not found in earlier versions of the library and is implemented on top of\n",
      "    [HDF5](http://www.hdfgroup.org/HDF5). This module can read and write\n",
      "    files in both the new netCDF 4 and the old netCDF 3 format, and can create\n",
      "    files that are readable by HDF5 clients. The API modelled after\n",
      "    [Scientific.IO.NetCDF](http://dirac.cnrs-orleans.fr/ScientificPython/),\n",
      "    and should be familiar to users of that module.\n",
      "    \n",
      "    Most new features of netCDF 4 are implemented, such as multiple\n",
      "    unlimited dimensions, groups and zlib data compression.  All the new\n",
      "    numeric data types (such as 64 bit and unsigned integer types) are\n",
      "    implemented. Compound (struct), variable length (vlen) and\n",
      "    enumerated (enum) data types are supported, but not the opaque data type.\n",
      "    Mixtures of compound, vlen and enum data types (such as\n",
      "    compound types containing enums, or vlens containing compound\n",
      "    types) are not supported.\n",
      "    \n",
      "    Download\n",
      "    ========\n",
      "    \n",
      "     - Latest bleeding-edge code from the\n",
      "       [github repository](http://github.com/Unidata/netcdf4-python).\n",
      "     - Latest [releases](https://pypi.python.org/pypi/netCDF4)\n",
      "       (source code and binary installers).\n",
      "    \n",
      "    Requires\n",
      "    ========\n",
      "    \n",
      "     - Python 2.7 or later (python 3 works too).\n",
      "     - [numpy array module](http://numpy.scipy.org), version 1.10.0 or later.\n",
      "     - [Cython](http://cython.org), version 0.21 or later.\n",
      "     - [setuptools](https://pypi.python.org/pypi/setuptools), version 18.0 or\n",
      "       later.\n",
      "     - [cftime](https://github.com/Unidata/cftime) for\n",
      "     the time and date handling utility functions (`netCDF4.num2date`,\n",
      "     `netCDF4.date2num` and `netCDF4.date2index`).\n",
      "     - The HDF5 C library version 1.8.4-patch1 or higher (1.8.x recommended)\n",
      "     from [](ftp://ftp.hdfgroup.org/HDF5/current/src).\n",
      "     ***netCDF version 4.4.1 or higher is recommended if using HDF5 1.10.x -\n",
      "     otherwise resulting files may be unreadable by clients using earlier\n",
      "     versions of HDF5.  For netCDF < 4.4.1, HDF5 version 1.8.x is recommended.***\n",
      "     Be sure to build with `--enable-hl --enable-shared`.\n",
      "     - [Libcurl](http://curl.haxx.se/libcurl), if you want\n",
      "     [OPeNDAP](http://opendap.org) support.\n",
      "     - [HDF4](http://www.hdfgroup.org/products/hdf4), if you want\n",
      "     to be able to read HDF4 \"Scientific Dataset\" (SD) files.\n",
      "     - The netCDF-4 C library from the [github releases\n",
      "       page](https://github.com/Unidata/netcdf-c/releases).\n",
      "     Version 4.1.1 or higher is required (4.2 or higher recommended).\n",
      "     Be sure to build with `--enable-netcdf-4 --enable-shared`, and set\n",
      "     `CPPFLAGS=\"-I $HDF5_DIR/include\"` and `LDFLAGS=\"-L $HDF5_DIR/lib\"`,\n",
      "     where `$HDF5_DIR` is the directory where HDF5 was installed.\n",
      "     If you want [OPeNDAP](http://opendap.org) support, add `--enable-dap`.\n",
      "     If you want HDF4 SD support, add `--enable-hdf4` and add\n",
      "     the location of the HDF4 headers and library to `$CPPFLAGS` and `$LDFLAGS`.\n",
      "     - for MPI parallel IO support, an MPI-enabled versions of the netcdf library\n",
      "     is required, as is the [mpi4py](http://mpi4py.scipy.org) python module.\n",
      "     Parallel IO further depends on the existence of MPI-enabled HDF5 or the\n",
      "     [PnetCDF](https://parallel-netcdf.github.io/) library.\n",
      "    \n",
      "    \n",
      "    Install\n",
      "    =======\n",
      "    \n",
      "     - install the requisite python modules and C libraries (see above). It's\n",
      "     easiest if all the C libs are built as shared libraries.\n",
      "     - By default, the utility `nc-config`, installed with netcdf 4.1.2 or higher,\n",
      "     will be run used to determine where all the dependencies live.\n",
      "     - If `nc-config` is not in your default `$PATH`\n",
      "     edit the `setup.cfg` file\n",
      "     in a text editor and follow the instructions in the comments.\n",
      "     In addition to specifying the path to `nc-config`,\n",
      "     you can manually set the paths to all the libraries and their include files\n",
      "     (in case `nc-config` does not do the right thing).\n",
      "     - run `python setup.py build`, then `python setup.py install` (as root if\n",
      "     necessary).\n",
      "     - [`pip install`](https://pip.pypa.io/en/latest/reference/pip_install.html) can\n",
      "     also be used, with library paths set with environment variables. To make\n",
      "     this work, the `USE_SETUPCFG` environment variable must be used to tell\n",
      "     setup.py not to use `setup.cfg`.\n",
      "     For example, `USE_SETUPCFG=0 HDF5_INCDIR=/usr/include/hdf5/serial\n",
      "     HDF5_LIBDIR=/usr/lib/x86_64-linux-gnu/hdf5/serial pip install` has been\n",
      "     shown to work on an Ubuntu/Debian linux system. Similarly, environment variables\n",
      "     (all capitalized) can be used to set the include and library paths for\n",
      "     `hdf5`, `netCDF4`, `hdf4`, `szip`, `jpeg`, `curl` and `zlib`. If the\n",
      "     libraries are installed in standard places (e.g. `/usr` or `/usr/local`),\n",
      "     the environment variables do not need to be set.\n",
      "     - run the tests in the 'test' directory by running `python run_all.py`.\n",
      "    \n",
      "    Tutorial\n",
      "    ========\n",
      "    \n",
      "    1. [Creating/Opening/Closing a netCDF file.](#section1)\n",
      "    2. [Groups in a netCDF file.](#section2)\n",
      "    3. [Dimensions in a netCDF file.](#section3)\n",
      "    4. [Variables in a netCDF file.](#section4)\n",
      "    5. [Attributes in a netCDF file.](#section5)\n",
      "    6. [Writing data to and retrieving data from a netCDF variable.](#section6)\n",
      "    7. [Dealing with time coordinates.](#section7)\n",
      "    8. [Reading data from a multi-file netCDF dataset.](#section8)\n",
      "    9. [Efficient compression of netCDF variables.](#section9)\n",
      "    10. [Beyond homogeneous arrays of a fixed type - compound data types.](#section10)\n",
      "    11. [Variable-length (vlen) data types.](#section11)\n",
      "    12. [Enum data type.](#section12)\n",
      "    13. [Parallel IO.](#section13)\n",
      "    14. [Dealing with strings.](#section14)\n",
      "    15. [In-memory (diskless) Datasets.](#section15)\n",
      "    \n",
      "    \n",
      "    ## <div id='section1'>1) Creating/Opening/Closing a netCDF file.\n",
      "    \n",
      "    To create a netCDF file from python, you simply call the `netCDF4.Dataset`\n",
      "    constructor. This is also the method used to open an existing netCDF\n",
      "    file.  If the file is open for write access (`mode='w', 'r+'` or `'a'`), you may\n",
      "    write any type of data including new dimensions, groups, variables and\n",
      "    attributes.  netCDF files come in five flavors (`NETCDF3_CLASSIC,\n",
      "    NETCDF3_64BIT_OFFSET, NETCDF3_64BIT_DATA, NETCDF4_CLASSIC`, and `NETCDF4`).\n",
      "    `NETCDF3_CLASSIC` was the original netcdf binary format, and was limited\n",
      "    to file sizes less than 2 Gb. `NETCDF3_64BIT_OFFSET` was introduced\n",
      "    in version 3.6.0 of the library, and extended the original binary format\n",
      "    to allow for file sizes greater than 2 Gb.\n",
      "    `NETCDF3_64BIT_DATA` is a new format that requires version 4.4.0 of\n",
      "    the C library - it extends the `NETCDF3_64BIT_OFFSET` binary format to\n",
      "    allow for unsigned/64 bit integer data types and 64-bit dimension sizes.\n",
      "    `NETCDF3_64BIT` is an alias for `NETCDF3_64BIT_OFFSET`.\n",
      "    `NETCDF4_CLASSIC` files use the version 4 disk format (HDF5), but omits features\n",
      "    not found in the version 3 API. They can be read by netCDF 3 clients\n",
      "    only if they have been relinked against the netCDF 4 library. They can\n",
      "    also be read by HDF5 clients. `NETCDF4` files use the version 4 disk\n",
      "    format (HDF5) and use the new features of the version 4 API.  The\n",
      "    `netCDF4` module can read and write files in any of these formats. When\n",
      "    creating a new file, the format may be specified using the `format`\n",
      "    keyword in the `Dataset` constructor.  The default format is\n",
      "    `NETCDF4`. To see how a given file is formatted, you can examine the\n",
      "    `data_model` attribute.  Closing the netCDF file is\n",
      "    accomplished via the `netCDF4.Dataset.close` method of the `netCDF4.Dataset`\n",
      "    instance.\n",
      "    \n",
      "    Here's an example:\n",
      "    \n",
      "        :::python\n",
      "        >>> from netCDF4 import Dataset\n",
      "        >>> rootgrp = Dataset(\"test.nc\", \"w\", format=\"NETCDF4\")\n",
      "        >>> print rootgrp.data_model\n",
      "        NETCDF4\n",
      "        >>> rootgrp.close()\n",
      "    \n",
      "    Remote [OPeNDAP](http://opendap.org)-hosted datasets can be accessed for\n",
      "    reading over http if a URL is provided to the `netCDF4.Dataset` constructor instead of a\n",
      "    filename.  However, this requires that the netCDF library be built with\n",
      "    OPenDAP support, via the `--enable-dap` configure option (added in\n",
      "    version 4.0.1).\n",
      "    \n",
      "    \n",
      "    ## <div id='section2'>2) Groups in a netCDF file.\n",
      "    \n",
      "    netCDF version 4 added support for organizing data in hierarchical\n",
      "    groups, which are analogous to directories in a filesystem. Groups serve\n",
      "    as containers for variables, dimensions and attributes, as well as other\n",
      "    groups.  A `netCDF4.Dataset` creates a special group, called\n",
      "    the 'root group', which is similar to the root directory in a unix\n",
      "    filesystem.  To create `netCDF4.Group` instances, use the\n",
      "    `netCDF4.Dataset.createGroup` method of a `netCDF4.Dataset` or `netCDF4.Group`\n",
      "    instance. `netCDF4.Dataset.createGroup` takes a single argument, a\n",
      "    python string containing the name of the new group. The new `netCDF4.Group`\n",
      "    instances contained within the root group can be accessed by name using\n",
      "    the `groups` dictionary attribute of the `netCDF4.Dataset` instance.  Only\n",
      "    `NETCDF4` formatted files support Groups, if you try to create a Group\n",
      "    in a netCDF 3 file you will get an error message.\n",
      "    \n",
      "        :::python\n",
      "        >>> rootgrp = Dataset(\"test.nc\", \"a\")\n",
      "        >>> fcstgrp = rootgrp.createGroup(\"forecasts\")\n",
      "        >>> analgrp = rootgrp.createGroup(\"analyses\")\n",
      "        >>> print rootgrp.groups\n",
      "        OrderedDict([(\"forecasts\",\n",
      "                      <netCDF4._netCDF4.Group object at 0x1b4b7b0>),\n",
      "                     (\"analyses\",\n",
      "                      <netCDF4._netCDF4.Group object at 0x1b4b970>)])\n",
      "    \n",
      "    Groups can exist within groups in a `netCDF4.Dataset`, just as directories\n",
      "    exist within directories in a unix filesystem. Each `netCDF4.Group` instance\n",
      "    has a `groups` attribute dictionary containing all of the group\n",
      "    instances contained within that group. Each `netCDF4.Group` instance also has a\n",
      "    `path` attribute that contains a simulated unix directory path to\n",
      "    that group.  To simplify the creation of nested groups, you can\n",
      "    use a unix-like path as an argument to `netCDF4.Dataset.createGroup`.\n",
      "    \n",
      "        :::python\n",
      "        >>> fcstgrp1 = rootgrp.createGroup(\"/forecasts/model1\")\n",
      "        >>> fcstgrp2 = rootgrp.createGroup(\"/forecasts/model2\")\n",
      "    \n",
      "    If any of the intermediate elements of the path do not exist, they are created,\n",
      "    just as with the unix command `'mkdir -p'`. If you try to create a group\n",
      "    that already exists, no error will be raised, and the existing group will be\n",
      "    returned.\n",
      "    \n",
      "    Here's an example that shows how to navigate all the groups in a\n",
      "    `netCDF4.Dataset`. The function `walktree` is a Python generator that is used\n",
      "    to walk the directory tree. Note that printing the `netCDF4.Dataset` or `netCDF4.Group`\n",
      "    object yields summary information about it's contents.\n",
      "    \n",
      "        :::python\n",
      "        >>> def walktree(top):\n",
      "        >>>     values = top.groups.values()\n",
      "        >>>     yield values\n",
      "        >>>     for value in top.groups.values():\n",
      "        >>>         for children in walktree(value):\n",
      "        >>>             yield children\n",
      "        >>> print rootgrp\n",
      "        >>> for children in walktree(rootgrp):\n",
      "        >>>      for child in children:\n",
      "        >>>          print child\n",
      "        <type \"netCDF4._netCDF4.Dataset\">\n",
      "        root group (NETCDF4 file format):\n",
      "            dimensions:\n",
      "            variables:\n",
      "            groups: forecasts, analyses\n",
      "        <type \"netCDF4._netCDF4.Group\">\n",
      "        group /forecasts:\n",
      "            dimensions:\n",
      "            variables:\n",
      "            groups: model1, model2\n",
      "        <type \"netCDF4._netCDF4.Group\">\n",
      "        group /analyses:\n",
      "            dimensions:\n",
      "            variables:\n",
      "            groups:\n",
      "        <type \"netCDF4._netCDF4.Group\">\n",
      "        group /forecasts/model1:\n",
      "            dimensions:\n",
      "            variables:\n",
      "            groups:\n",
      "        <type \"netCDF4._netCDF4.Group\">\n",
      "        group /forecasts/model2:\n",
      "            dimensions:\n",
      "            variables:\n",
      "            groups:\n",
      "    \n",
      "    ## <div id='section3'>3) Dimensions in a netCDF file.\n",
      "    \n",
      "    netCDF defines the sizes of all variables in terms of dimensions, so\n",
      "    before any variables can be created the dimensions they use must be\n",
      "    created first. A special case, not often used in practice, is that of a\n",
      "    scalar variable, which has no dimensions. A dimension is created using\n",
      "    the `netCDF4.Dataset.createDimension` method of a `netCDF4.Dataset`\n",
      "    or `netCDF4.Group` instance. A Python string is used to set the name of the\n",
      "    dimension, and an integer value is used to set the size. To create an\n",
      "    unlimited dimension (a dimension that can be appended to), the size\n",
      "    value is set to `None` or 0. In this example, there both the `time` and\n",
      "    `level` dimensions are unlimited.  Having more than one unlimited\n",
      "    dimension is a new netCDF 4 feature, in netCDF 3 files there may be only\n",
      "    one, and it must be the first (leftmost) dimension of the variable.\n",
      "    \n",
      "        :::python\n",
      "        >>> level = rootgrp.createDimension(\"level\", None)\n",
      "        >>> time = rootgrp.createDimension(\"time\", None)\n",
      "        >>> lat = rootgrp.createDimension(\"lat\", 73)\n",
      "        >>> lon = rootgrp.createDimension(\"lon\", 144)\n",
      "    \n",
      "    \n",
      "    All of the `netCDF4.Dimension` instances are stored in a python dictionary.\n",
      "    \n",
      "        :::python\n",
      "        >>> print rootgrp.dimensions\n",
      "        OrderedDict([(\"level\", <netCDF4._netCDF4.Dimension object at 0x1b48030>),\n",
      "                     (\"time\", <netCDF4._netCDF4.Dimension object at 0x1b481c0>),\n",
      "                     (\"lat\", <netCDF4._netCDF4.Dimension object at 0x1b480f8>),\n",
      "                     (\"lon\", <netCDF4._netCDF4.Dimension object at 0x1b48a08>)])\n",
      "    \n",
      "    Calling the python `len` function with a `netCDF4.Dimension` instance returns\n",
      "    the current size of that dimension.\n",
      "    The `netCDF4.Dimension.isunlimited` method of a `netCDF4.Dimension` instance\n",
      "    can be used to determine if the dimensions is unlimited, or appendable.\n",
      "    \n",
      "        :::python\n",
      "        >>> print len(lon)\n",
      "        144\n",
      "        >>> print lon.isunlimited()\n",
      "        False\n",
      "        >>> print time.isunlimited()\n",
      "        True\n",
      "    \n",
      "    Printing the `netCDF4.Dimension` object\n",
      "    provides useful summary info, including the name and length of the dimension,\n",
      "    and whether it is unlimited.\n",
      "    \n",
      "        :::python\n",
      "        >>> for dimobj in rootgrp.dimensions.values():\n",
      "        >>>    print dimobj\n",
      "        <type \"netCDF4._netCDF4.Dimension\"> (unlimited): name = \"level\", size = 0\n",
      "        <type \"netCDF4._netCDF4.Dimension\"> (unlimited): name = \"time\", size = 0\n",
      "        <type \"netCDF4._netCDF4.Dimension\">: name = \"lat\", size = 73\n",
      "        <type \"netCDF4._netCDF4.Dimension\">: name = \"lon\", size = 144\n",
      "        <type \"netCDF4._netCDF4.Dimension\"> (unlimited): name = \"time\", size = 0\n",
      "    \n",
      "    `netCDF4.Dimension` names can be changed using the\n",
      "    `netCDF4.Datatset.renameDimension` method of a `netCDF4.Dataset` or\n",
      "    `netCDF4.Group` instance.\n",
      "    \n",
      "    ## <div id='section4'>4) Variables in a netCDF file.\n",
      "    \n",
      "    netCDF variables behave much like python multidimensional array objects\n",
      "    supplied by the [numpy module](http://numpy.scipy.org). However,\n",
      "    unlike numpy arrays, netCDF4 variables can be appended to along one or\n",
      "    more 'unlimited' dimensions. To create a netCDF variable, use the\n",
      "    `netCDF4.Dataset.createVariable` method of a `netCDF4.Dataset` or\n",
      "    `netCDF4.Group` instance. The `netCDF4.Dataset.createVariable` method\n",
      "    has two mandatory arguments, the variable name (a Python string), and\n",
      "    the variable datatype. The variable's dimensions are given by a tuple\n",
      "    containing the dimension names (defined previously with\n",
      "    `netCDF4.Dataset.createDimension`). To create a scalar\n",
      "    variable, simply leave out the dimensions keyword. The variable\n",
      "    primitive datatypes correspond to the dtype attribute of a numpy array.\n",
      "    You can specify the datatype as a numpy dtype object, or anything that\n",
      "    can be converted to a numpy dtype object.  Valid datatype specifiers\n",
      "    include: `'f4'` (32-bit floating point), `'f8'` (64-bit floating\n",
      "    point), `'i4'` (32-bit signed integer), `'i2'` (16-bit signed\n",
      "    integer), `'i8'` (64-bit signed integer), `'i1'` (8-bit signed\n",
      "    integer), `'u1'` (8-bit unsigned integer), `'u2'` (16-bit unsigned\n",
      "    integer), `'u4'` (32-bit unsigned integer), `'u8'` (64-bit unsigned\n",
      "    integer), or `'S1'` (single-character string).  The old Numeric\n",
      "    single-character typecodes (`'f'`,`'d'`,`'h'`,\n",
      "    `'s'`,`'b'`,`'B'`,`'c'`,`'i'`,`'l'`), corresponding to\n",
      "    (`'f4'`,`'f8'`,`'i2'`,`'i2'`,`'i1'`,`'i1'`,`'S1'`,`'i4'`,`'i4'`),\n",
      "    will also work. The unsigned integer types and the 64-bit integer type\n",
      "    can only be used if the file format is `NETCDF4`.\n",
      "    \n",
      "    The dimensions themselves are usually also defined as variables, called\n",
      "    coordinate variables. The `netCDF4.Dataset.createVariable`\n",
      "    method returns an instance of the `netCDF4.Variable` class whose methods can be\n",
      "    used later to access and set variable data and attributes.\n",
      "    \n",
      "        :::python\n",
      "        >>> times = rootgrp.createVariable(\"time\",\"f8\",(\"time\",))\n",
      "        >>> levels = rootgrp.createVariable(\"level\",\"i4\",(\"level\",))\n",
      "        >>> latitudes = rootgrp.createVariable(\"lat\",\"f4\",(\"lat\",))\n",
      "        >>> longitudes = rootgrp.createVariable(\"lon\",\"f4\",(\"lon\",))\n",
      "        >>> # two dimensions unlimited\n",
      "        >>> temp = rootgrp.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\n",
      "    \n",
      "    To get summary info on a `netCDF4.Variable` instance in an interactive session, just print it.\n",
      "    \n",
      "        :::python\n",
      "        >>> print temp\n",
      "        <type \"netCDF4._netCDF4.Variable\">\n",
      "        float32 temp(time, level, lat, lon)\n",
      "            least_significant_digit: 3\n",
      "            units: K\n",
      "        unlimited dimensions: time, level\n",
      "        current shape = (0, 0, 73, 144)\n",
      "    \n",
      "    You can use a path to create a Variable inside a hierarchy of groups.\n",
      "    \n",
      "        :::python\n",
      "        >>> ftemp = rootgrp.createVariable(\"/forecasts/model1/temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\n",
      "    \n",
      "    If the intermediate groups do not yet exist, they will be created.\n",
      "    \n",
      "    You can also query a `netCDF4.Dataset` or `netCDF4.Group` instance directly to obtain `netCDF4.Group` or\n",
      "    `netCDF4.Variable` instances using paths.\n",
      "    \n",
      "        :::python\n",
      "        >>> print rootgrp[\"/forecasts/model1\"] # a Group instance\n",
      "        <type \"netCDF4._netCDF4.Group\">\n",
      "        group /forecasts/model1:\n",
      "            dimensions(sizes):\n",
      "            variables(dimensions): float32 temp(time,level,lat,lon)\n",
      "            groups:\n",
      "        >>> print rootgrp[\"/forecasts/model1/temp\"] # a Variable instance\n",
      "        <type \"netCDF4._netCDF4.Variable\">\n",
      "        float32 temp(time, level, lat, lon)\n",
      "        path = /forecasts/model1\n",
      "        unlimited dimensions: time, level\n",
      "        current shape = (0, 0, 73, 144)\n",
      "        filling on, default _FillValue of 9.96920996839e+36 used\n",
      "    \n",
      "    All of the variables in the `netCDF4.Dataset` or `netCDF4.Group` are stored in a\n",
      "    Python dictionary, in the same way as the dimensions:\n",
      "    \n",
      "        :::python\n",
      "        >>> print rootgrp.variables\n",
      "        OrderedDict([(\"time\", <netCDF4.Variable object at 0x1b4ba70>),\n",
      "                     (\"level\", <netCDF4.Variable object at 0x1b4bab0>),\n",
      "                     (\"lat\", <netCDF4.Variable object at 0x1b4baf0>),\n",
      "                     (\"lon\", <netCDF4.Variable object at 0x1b4bb30>),\n",
      "                     (\"temp\", <netCDF4.Variable object at 0x1b4bb70>)])\n",
      "    \n",
      "    `netCDF4.Variable` names can be changed using the\n",
      "    `netCDF4.Dataset.renameVariable` method of a `netCDF4.Dataset`\n",
      "    instance.\n",
      "    \n",
      "    \n",
      "    ## <div id='section5'>5) Attributes in a netCDF file.\n",
      "    \n",
      "    There are two types of attributes in a netCDF file, global and variable.\n",
      "    Global attributes provide information about a group, or the entire\n",
      "    dataset, as a whole. `netCDF4.Variable` attributes provide information about\n",
      "    one of the variables in a group. Global attributes are set by assigning\n",
      "    values to `netCDF4.Dataset` or `netCDF4.Group` instance variables. `netCDF4.Variable`\n",
      "    attributes are set by assigning values to `netCDF4.Variable` instances\n",
      "    variables. Attributes can be strings, numbers or sequences. Returning to\n",
      "    our example,\n",
      "    \n",
      "        :::python\n",
      "        >>> import time\n",
      "        >>> rootgrp.description = \"bogus example script\"\n",
      "        >>> rootgrp.history = \"Created \" + time.ctime(time.time())\n",
      "        >>> rootgrp.source = \"netCDF4 python module tutorial\"\n",
      "        >>> latitudes.units = \"degrees north\"\n",
      "        >>> longitudes.units = \"degrees east\"\n",
      "        >>> levels.units = \"hPa\"\n",
      "        >>> temp.units = \"K\"\n",
      "        >>> times.units = \"hours since 0001-01-01 00:00:00.0\"\n",
      "        >>> times.calendar = \"gregorian\"\n",
      "    \n",
      "    The `netCDF4.Dataset.ncattrs` method of a `netCDF4.Dataset`, `netCDF4.Group` or\n",
      "    `netCDF4.Variable` instance can be used to retrieve the names of all the netCDF\n",
      "    attributes. This method is provided as a convenience, since using the\n",
      "    built-in `dir` Python function will return a bunch of private methods\n",
      "    and attributes that cannot (or should not) be modified by the user.\n",
      "    \n",
      "        :::python\n",
      "        >>> for name in rootgrp.ncattrs():\n",
      "        >>>     print \"Global attr\", name, \"=\", getattr(rootgrp,name)\n",
      "        Global attr description = bogus example script\n",
      "        Global attr history = Created Mon Nov  7 10.30:56 2005\n",
      "        Global attr source = netCDF4 python module tutorial\n",
      "    \n",
      "    The `__dict__` attribute of a `netCDF4.Dataset`, `netCDF4.Group` or `netCDF4.Variable`\n",
      "    instance provides all the netCDF attribute name/value pairs in a python\n",
      "    dictionary:\n",
      "    \n",
      "        :::python\n",
      "        >>> print rootgrp.__dict__\n",
      "        OrderedDict([(u\"description\", u\"bogus example script\"),\n",
      "                     (u\"history\", u\"Created Thu Mar  3 19:30:33 2011\"),\n",
      "                     (u\"source\", u\"netCDF4 python module tutorial\")])\n",
      "    \n",
      "    Attributes can be deleted from a netCDF `netCDF4.Dataset`, `netCDF4.Group` or\n",
      "    `netCDF4.Variable` using the python `del` statement (i.e. `del grp.foo`\n",
      "    removes the attribute `foo` the the group `grp`).\n",
      "    \n",
      "    ## <div id='section6'>6) Writing data to and retrieving data from a netCDF variable.\n",
      "    \n",
      "    Now that you have a netCDF `netCDF4.Variable` instance, how do you put data\n",
      "    into it? You can just treat it like an array and assign data to a slice.\n",
      "    \n",
      "        :::python\n",
      "        >>> import numpy\n",
      "        >>> lats =  numpy.arange(-90,91,2.5)\n",
      "        >>> lons =  numpy.arange(-180,180,2.5)\n",
      "        >>> latitudes[:] = lats\n",
      "        >>> longitudes[:] = lons\n",
      "        >>> print \"latitudes =\\n\",latitudes[:]\n",
      "        latitudes =\n",
      "        [-90.  -87.5 -85.  -82.5 -80.  -77.5 -75.  -72.5 -70.  -67.5 -65.  -62.5\n",
      "         -60.  -57.5 -55.  -52.5 -50.  -47.5 -45.  -42.5 -40.  -37.5 -35.  -32.5\n",
      "         -30.  -27.5 -25.  -22.5 -20.  -17.5 -15.  -12.5 -10.   -7.5  -5.   -2.5\n",
      "           0.    2.5   5.    7.5  10.   12.5  15.   17.5  20.   22.5  25.   27.5\n",
      "          30.   32.5  35.   37.5  40.   42.5  45.   47.5  50.   52.5  55.   57.5\n",
      "          60.   62.5  65.   67.5  70.   72.5  75.   77.5  80.   82.5  85.   87.5\n",
      "          90. ]\n",
      "    \n",
      "    Unlike NumPy's array objects, netCDF `netCDF4.Variable`\n",
      "    objects with unlimited dimensions will grow along those dimensions if you\n",
      "    assign data outside the currently defined range of indices.\n",
      "    \n",
      "        :::python\n",
      "        >>> # append along two unlimited dimensions by assigning to slice.\n",
      "        >>> nlats = len(rootgrp.dimensions[\"lat\"])\n",
      "        >>> nlons = len(rootgrp.dimensions[\"lon\"])\n",
      "        >>> print \"temp shape before adding data = \",temp.shape\n",
      "        temp shape before adding data =  (0, 0, 73, 144)\n",
      "        >>>\n",
      "        >>> from numpy.random import uniform\n",
      "        >>> temp[0:5,0:10,:,:] = uniform(size=(5,10,nlats,nlons))\n",
      "        >>> print \"temp shape after adding data = \",temp.shape\n",
      "        temp shape after adding data =  (6, 10, 73, 144)\n",
      "        >>>\n",
      "        >>> # levels have grown, but no values yet assigned.\n",
      "        >>> print \"levels shape after adding pressure data = \",levels.shape\n",
      "        levels shape after adding pressure data =  (10,)\n",
      "    \n",
      "    Note that the size of the levels variable grows when data is appended\n",
      "    along the `level` dimension of the variable `temp`, even though no\n",
      "    data has yet been assigned to levels.\n",
      "    \n",
      "        :::python\n",
      "        >>> # now, assign data to levels dimension variable.\n",
      "        >>> levels[:] =  [1000.,850.,700.,500.,300.,250.,200.,150.,100.,50.]\n",
      "    \n",
      "    However, that there are some differences between NumPy and netCDF\n",
      "    variable slicing rules. Slices behave as usual, being specified as a\n",
      "    `start:stop:step` triplet. Using a scalar integer index `i` takes the ith\n",
      "    element and reduces the rank of the output array by one. Boolean array and\n",
      "    integer sequence indexing behaves differently for netCDF variables\n",
      "    than for numpy arrays.  Only 1-d boolean arrays and integer sequences are\n",
      "    allowed, and these indices work independently along each dimension (similar\n",
      "    to the way vector subscripts work in fortran).  This means that\n",
      "    \n",
      "        :::python\n",
      "        >>> temp[0, 0, [0,1,2,3], [0,1,2,3]]\n",
      "    \n",
      "    returns an array of shape (4,4) when slicing a netCDF variable, but for a\n",
      "    numpy array it returns an array of shape (4,).\n",
      "    Similarly, a netCDF variable of shape `(2,3,4,5)` indexed\n",
      "    with `[0, array([True, False, True]), array([False, True, True, True]), :]`\n",
      "    would return a `(2, 3, 5)` array. In NumPy, this would raise an error since\n",
      "    it would be equivalent to `[0, [0,1], [1,2,3], :]`. When slicing with integer\n",
      "    sequences, the indices ***need not be sorted*** and ***may contain\n",
      "    duplicates*** (both of these are new features in version 1.2.1).\n",
      "    While this behaviour may cause some confusion for those used to NumPy's 'fancy indexing' rules,\n",
      "    it provides a very powerful way to extract data from multidimensional netCDF\n",
      "    variables by using logical operations on the dimension arrays to create slices.\n",
      "    \n",
      "    For example,\n",
      "    \n",
      "        :::python\n",
      "        >>> tempdat = temp[::2, [1,3,6], lats>0, lons>0]\n",
      "    \n",
      "    will extract time indices 0,2 and 4, pressure levels\n",
      "    850, 500 and 200 hPa, all Northern Hemisphere latitudes and Eastern\n",
      "    Hemisphere longitudes, resulting in a numpy array of shape  (3, 3, 36, 71).\n",
      "    \n",
      "        :::python\n",
      "        >>> print \"shape of fancy temp slice = \",tempdat.shape\n",
      "        shape of fancy temp slice =  (3, 3, 36, 71)\n",
      "    \n",
      "    ***Special note for scalar variables***: To extract data from a scalar variable\n",
      "    `v` with no associated dimensions, use `numpy.asarray(v)` or `v[...]`. The result\n",
      "    will be a numpy scalar array.\n",
      "    \n",
      "    By default, netcdf4-python returns numpy masked arrays with values equal to the\n",
      "    `missing_value` or `_FillValue` variable attributes masked.  The\n",
      "    `netCDF4.Dataset.set_auto_mask`  `netCDF4.Dataset` and `netCDF4.Variable` methods\n",
      "    can be used to disable this feature so that\n",
      "    numpy arrays are always returned, with the missing values included. Prior to\n",
      "    version 1.4.0 the default behavior was to only return masked arrays when the\n",
      "    requested slice contained missing values.  This behavior can be recovered\n",
      "    using the `netCDF4.Dataset.set_always_mask` method. If a masked array is\n",
      "    written to a netCDF variable, the masked elements are filled with the\n",
      "    value specified by the `missing_value` attribute.  If the variable has\n",
      "    no `missing_value`, the `_FillValue` is used instead.\n",
      "    \n",
      "    ## <div id='section7'>7) Dealing with time coordinates.\n",
      "    \n",
      "    Time coordinate values pose a special challenge to netCDF users.  Most\n",
      "    metadata standards (such as CF) specify that time should be\n",
      "    measure relative to a fixed date using a certain calendar, with units\n",
      "    specified like `hours since YY-MM-DD hh:mm:ss`.  These units can be\n",
      "    awkward to deal with, without a utility to convert the values to and\n",
      "    from calendar dates.  The function called `netCDF4.num2date` and `netCDF4.date2num` are\n",
      "    provided with this package to do just that (starting with version 1.4.0, the\n",
      "    [cftime](https://unidata.github.io/cftime) package must be installed\n",
      "    separately).  Here's an example of how they\n",
      "    can be used:\n",
      "    \n",
      "        :::python\n",
      "        >>> # fill in times.\n",
      "        >>> from datetime import datetime, timedelta\n",
      "        >>> from netCDF4 import num2date, date2num\n",
      "        >>> dates = [datetime(2001,3,1)+n*timedelta(hours=12) for n in range(temp.shape[0])]\n",
      "        >>> times[:] = date2num(dates,units=times.units,calendar=times.calendar)\n",
      "        >>> print \"time values (in units %s): \" % times.units+\"\\n\",times[:]\n",
      "        time values (in units hours since January 1, 0001):\n",
      "        [ 17533056.  17533068.  17533080.  17533092.  17533104.]\n",
      "        >>> dates = num2date(times[:],units=times.units,calendar=times.calendar)\n",
      "        >>> print \"dates corresponding to time values:\\n\",dates\n",
      "        dates corresponding to time values:\n",
      "        [2001-03-01 00:00:00 2001-03-01 12:00:00 2001-03-02 00:00:00\n",
      "         2001-03-02 12:00:00 2001-03-03 00:00:00]\n",
      "    \n",
      "    `netCDF4.num2date` converts numeric values of time in the specified `units`\n",
      "    and `calendar` to datetime objects, and `netCDF4.date2num` does the reverse.\n",
      "    All the calendars currently defined in the\n",
      "    [CF metadata convention](http://cfconventions.org) are supported.\n",
      "    A function called `netCDF4.date2index` is also provided which returns the indices\n",
      "    of a netCDF time variable corresponding to a sequence of datetime instances.\n",
      "    \n",
      "    \n",
      "    ## <div id='section8'>8) Reading data from a multi-file netCDF dataset.\n",
      "    \n",
      "    If you want to read data from a variable that spans multiple netCDF files,\n",
      "    you can use the `netCDF4.MFDataset` class to read the data as if it were\n",
      "    contained in a single file. Instead of using a single filename to create\n",
      "    a `netCDF4.Dataset` instance, create a `netCDF4.MFDataset` instance with either a list\n",
      "    of filenames, or a string with a wildcard (which is then converted to\n",
      "    a sorted list of files using the python glob module).\n",
      "    Variables in the list of files that share the same unlimited\n",
      "    dimension are aggregated together, and can be sliced across multiple\n",
      "    files.  To illustrate this, let's first create a bunch of netCDF files with\n",
      "    the same variable (with the same unlimited dimension).  The files\n",
      "    must in be in `NETCDF3_64BIT_OFFSET`, `NETCDF3_64BIT_DATA`, `NETCDF3_CLASSIC` or\n",
      "    `NETCDF4_CLASSIC` format (`NETCDF4` formatted multi-file\n",
      "    datasets are not supported).\n",
      "    \n",
      "        :::python\n",
      "        >>> for nf in range(10):\n",
      "        >>>     f = Dataset(\"mftest%s.nc\" % nf,\"w\")\n",
      "        >>>     f.createDimension(\"x\",None)\n",
      "        >>>     x = f.createVariable(\"x\",\"i\",(\"x\",))\n",
      "        >>>     x[0:10] = numpy.arange(nf*10,10*(nf+1))\n",
      "        >>>     f.close()\n",
      "    \n",
      "    Now read all the files back in at once with `netCDF4.MFDataset`\n",
      "    \n",
      "        :::python\n",
      "        >>> from netCDF4 import MFDataset\n",
      "        >>> f = MFDataset(\"mftest*nc\")\n",
      "        >>> print f.variables[\"x\"][:]\n",
      "        [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      "         25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      "         50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      "         75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
      "    \n",
      "    Note that `netCDF4.MFDataset` can only be used to read, not write, multi-file\n",
      "    datasets.\n",
      "    \n",
      "    ## <div id='section9'>9) Efficient compression of netCDF variables.\n",
      "    \n",
      "    Data stored in netCDF 4 `netCDF4.Variable` objects can be compressed and\n",
      "    decompressed on the fly. The parameters for the compression are\n",
      "    determined by the `zlib`, `complevel` and `shuffle` keyword arguments\n",
      "    to the `netCDF4.Dataset.createVariable` method. To turn on\n",
      "    compression, set `zlib=True`.  The `complevel` keyword regulates the\n",
      "    speed and efficiency of the compression (1 being fastest, but lowest\n",
      "    compression ratio, 9 being slowest but best compression ratio). The\n",
      "    default value of `complevel` is 4. Setting `shuffle=False` will turn\n",
      "    off the HDF5 shuffle filter, which de-interlaces a block of data before\n",
      "    compression by reordering the bytes.  The shuffle filter can\n",
      "    significantly improve compression ratios, and is on by default.  Setting\n",
      "    `fletcher32` keyword argument to\n",
      "    `netCDF4.Dataset.createVariable` to `True` (it's `False` by\n",
      "    default) enables the Fletcher32 checksum algorithm for error detection.\n",
      "    It's also possible to set the HDF5 chunking parameters and endian-ness\n",
      "    of the binary data stored in the HDF5 file with the `chunksizes`\n",
      "    and `endian` keyword arguments to\n",
      "    `netCDF4.Dataset.createVariable`.  These keyword arguments only\n",
      "    are relevant for `NETCDF4` and `NETCDF4_CLASSIC` files (where the\n",
      "    underlying file format is HDF5) and are silently ignored if the file\n",
      "    format is `NETCDF3_CLASSIC`, `NETCDF3_64BIT_OFFSET` or `NETCDF3_64BIT_DATA`.\n",
      "    \n",
      "    If your data only has a certain number of digits of precision (say for\n",
      "    example, it is temperature data that was measured with a precision of\n",
      "    0.1 degrees), you can dramatically improve zlib compression by\n",
      "    quantizing (or truncating) the data using the `least_significant_digit`\n",
      "    keyword argument to `netCDF4.Dataset.createVariable`. The least\n",
      "    significant digit is the power of ten of the smallest decimal place in\n",
      "    the data that is a reliable value. For example if the data has a\n",
      "    precision of 0.1, then setting `least_significant_digit=1` will cause\n",
      "    data the data to be quantized using `numpy.around(scale*data)/scale`, where\n",
      "    scale = 2**bits, and bits is determined so that a precision of 0.1 is\n",
      "    retained (in this case bits=4).  Effectively, this makes the compression\n",
      "    'lossy' instead of 'lossless', that is some precision in the data is\n",
      "    sacrificed for the sake of disk space.\n",
      "    \n",
      "    In our example, try replacing the line\n",
      "    \n",
      "        :::python\n",
      "        >>> temp = rootgrp.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\n",
      "    \n",
      "    with\n",
      "    \n",
      "        :::python\n",
      "        >>> temp = dataset.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",),zlib=True)\n",
      "    \n",
      "    and then\n",
      "    \n",
      "        :::python\n",
      "        >>> temp = dataset.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",),zlib=True,least_significant_digit=3)\n",
      "    \n",
      "    and see how much smaller the resulting files are.\n",
      "    \n",
      "    ## <div id='section10'>10) Beyond homogeneous arrays of a fixed type - compound data types.\n",
      "    \n",
      "    Compound data types map directly to numpy structured (a.k.a 'record')\n",
      "    arrays.  Structured arrays are akin to C structs, or derived types\n",
      "    in Fortran. They allow for the construction of table-like structures\n",
      "    composed of combinations of other data types, including other\n",
      "    compound types. Compound types might be useful for representing multiple\n",
      "    parameter values at each point on a grid, or at each time and space\n",
      "    location for scattered (point) data. You can then access all the\n",
      "    information for a point by reading one variable, instead of reading\n",
      "    different parameters from different variables.  Compound data types\n",
      "    are created from the corresponding numpy data type using the\n",
      "    `netCDF4.Dataset.createCompoundType` method of a `netCDF4.Dataset` or `netCDF4.Group` instance.\n",
      "    Since there is no native complex data type in netcdf, compound types are handy\n",
      "    for storing numpy complex arrays.  Here's an example:\n",
      "    \n",
      "        :::python\n",
      "        >>> f = Dataset(\"complex.nc\",\"w\")\n",
      "        >>> size = 3 # length of 1-d complex array\n",
      "        >>> # create sample complex data.\n",
      "        >>> datac = numpy.exp(1j*(1.+numpy.linspace(0, numpy.pi, size)))\n",
      "        >>> # create complex128 compound data type.\n",
      "        >>> complex128 = numpy.dtype([(\"real\",numpy.float64),(\"imag\",numpy.float64)])\n",
      "        >>> complex128_t = f.createCompoundType(complex128,\"complex128\")\n",
      "        >>> # create a variable with this data type, write some data to it.\n",
      "        >>> f.createDimension(\"x_dim\",None)\n",
      "        >>> v = f.createVariable(\"cmplx_var\",complex128_t,\"x_dim\")\n",
      "        >>> data = numpy.empty(size,complex128) # numpy structured array\n",
      "        >>> data[\"real\"] = datac.real; data[\"imag\"] = datac.imag\n",
      "        >>> v[:] = data # write numpy structured array to netcdf compound var\n",
      "        >>> # close and reopen the file, check the contents.\n",
      "        >>> f.close(); f = Dataset(\"complex.nc\")\n",
      "        >>> v = f.variables[\"cmplx_var\"]\n",
      "        >>> datain = v[:] # read in all the data into a numpy structured array\n",
      "        >>> # create an empty numpy complex array\n",
      "        >>> datac2 = numpy.empty(datain.shape,numpy.complex128)\n",
      "        >>> # .. fill it with contents of structured array.\n",
      "        >>> datac2.real = datain[\"real\"]; datac2.imag = datain[\"imag\"]\n",
      "        >>> print datac.dtype,datac # original data\n",
      "        complex128 [ 0.54030231+0.84147098j -0.84147098+0.54030231j  -0.54030231-0.84147098j]\n",
      "        >>>\n",
      "        >>> print datac2.dtype,datac2 # data from file\n",
      "        complex128 [ 0.54030231+0.84147098j -0.84147098+0.54030231j  -0.54030231-0.84147098j]\n",
      "    \n",
      "    Compound types can be nested, but you must create the 'inner'\n",
      "    ones first. All possible numpy structured arrays cannot be\n",
      "    represented as Compound variables - an error message will be\n",
      "    raise if you try to create one that is not supported.\n",
      "    All of the compound types defined for a `netCDF4.Dataset` or `netCDF4.Group` are stored\n",
      "    in a Python dictionary, just like variables and dimensions. As always, printing\n",
      "    objects gives useful summary information in an interactive session:\n",
      "    \n",
      "        :::python\n",
      "        >>> print f\n",
      "        <type \"netCDF4._netCDF4.Dataset\">\n",
      "        root group (NETCDF4 file format):\n",
      "            dimensions: x_dim\n",
      "            variables: cmplx_var\n",
      "            groups:\n",
      "        <type \"netCDF4._netCDF4.Variable\">\n",
      "        >>> print f.variables[\"cmplx_var\"]\n",
      "        compound cmplx_var(x_dim)\n",
      "        compound data type: [(\"real\", \"<f8\"), (\"imag\", \"<f8\")]\n",
      "        unlimited dimensions: x_dim\n",
      "        current shape = (3,)\n",
      "        >>> print f.cmptypes\n",
      "        OrderedDict([(\"complex128\", <netCDF4.CompoundType object at 0x1029eb7e8>)])\n",
      "        >>> print f.cmptypes[\"complex128\"]\n",
      "        <type \"netCDF4._netCDF4.CompoundType\">: name = \"complex128\", numpy dtype = [(u\"real\",\"<f8\"), (u\"imag\", \"<f8\")]\n",
      "    \n",
      "    ## <div id='section11'>11) Variable-length (vlen) data types.\n",
      "    \n",
      "    NetCDF 4 has support for variable-length or \"ragged\" arrays.  These are arrays\n",
      "    of variable length sequences having the same type. To create a variable-length\n",
      "    data type, use the `netCDF4.Dataset.createVLType` method\n",
      "    method of a `netCDF4.Dataset` or `netCDF4.Group` instance.\n",
      "    \n",
      "        :::python\n",
      "        >>> f = Dataset(\"tst_vlen.nc\",\"w\")\n",
      "        >>> vlen_t = f.createVLType(numpy.int32, \"phony_vlen\")\n",
      "    \n",
      "    The numpy datatype of the variable-length sequences and the name of the\n",
      "    new datatype must be specified. Any of the primitive datatypes can be\n",
      "    used (signed and unsigned integers, 32 and 64 bit floats, and characters),\n",
      "    but compound data types cannot.\n",
      "    A new variable can then be created using this datatype.\n",
      "    \n",
      "        :::python\n",
      "        >>> x = f.createDimension(\"x\",3)\n",
      "        >>> y = f.createDimension(\"y\",4)\n",
      "        >>> vlvar = f.createVariable(\"phony_vlen_var\", vlen_t, (\"y\",\"x\"))\n",
      "    \n",
      "    Since there is no native vlen datatype in numpy, vlen arrays are represented\n",
      "    in python as object arrays (arrays of dtype `object`). These are arrays whose\n",
      "    elements are Python object pointers, and can contain any type of python object.\n",
      "    For this application, they must contain 1-D numpy arrays all of the same type\n",
      "    but of varying length.\n",
      "    In this case, they contain 1-D numpy `int32` arrays of random length between\n",
      "    1 and 10.\n",
      "    \n",
      "        :::python\n",
      "        >>> import random\n",
      "        >>> data = numpy.empty(len(y)*len(x),object)\n",
      "        >>> for n in range(len(y)*len(x)):\n",
      "        >>>    data[n] = numpy.arange(random.randint(1,10),dtype=\"int32\")+1\n",
      "        >>> data = numpy.reshape(data,(len(y),len(x)))\n",
      "        >>> vlvar[:] = data\n",
      "        >>> print \"vlen variable =\\n\",vlvar[:]\n",
      "        vlen variable =\n",
      "        [[[ 1  2  3  4  5  6  7  8  9 10] [1 2 3 4 5] [1 2 3 4 5 6 7 8]]\n",
      "         [[1 2 3 4 5 6 7] [1 2 3 4 5 6] [1 2 3 4 5]]\n",
      "         [[1 2 3 4 5] [1 2 3 4] [1]]\n",
      "         [[ 1  2  3  4  5  6  7  8  9 10] [ 1  2  3  4  5  6  7  8  9 10]\n",
      "          [1 2 3 4 5 6 7 8]]]\n",
      "        >>> print f\n",
      "        <type \"netCDF4._netCDF4.Dataset\">\n",
      "        root group (NETCDF4 file format):\n",
      "            dimensions: x, y\n",
      "            variables: phony_vlen_var\n",
      "            groups:\n",
      "        >>> print f.variables[\"phony_vlen_var\"]\n",
      "        <type \"netCDF4._netCDF4.Variable\">\n",
      "        vlen phony_vlen_var(y, x)\n",
      "        vlen data type: int32\n",
      "        unlimited dimensions:\n",
      "        current shape = (4, 3)\n",
      "        >>> print f.VLtypes[\"phony_vlen\"]\n",
      "        <type \"netCDF4._netCDF4.VLType\">: name = \"phony_vlen\", numpy dtype = int32\n",
      "    \n",
      "    Numpy object arrays containing python strings can also be written as vlen\n",
      "    variables,  For vlen strings, you don't need to create a vlen data type.\n",
      "    Instead, simply use the python `str` builtin (or a numpy string datatype\n",
      "    with fixed length greater than 1) when calling the\n",
      "    `netCDF4.Dataset.createVariable` method.\n",
      "    \n",
      "        :::python\n",
      "        >>> z = f.createDimension(\"z\",10)\n",
      "        >>> strvar = rootgrp.createVariable(\"strvar\", str, \"z\")\n",
      "    \n",
      "    In this example, an object array is filled with random python strings with\n",
      "    random lengths between 2 and 12 characters, and the data in the object\n",
      "    array is assigned to the vlen string variable.\n",
      "    \n",
      "        :::python\n",
      "        >>> chars = \"1234567890aabcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
      "        >>> data = numpy.empty(10,\"O\")\n",
      "        >>> for n in range(10):\n",
      "        >>>     stringlen = random.randint(2,12)\n",
      "        >>>     data[n] = \"\".join([random.choice(chars) for i in range(stringlen)])\n",
      "        >>> strvar[:] = data\n",
      "        >>> print \"variable-length string variable:\\n\",strvar[:]\n",
      "        variable-length string variable:\n",
      "        [aDy29jPt 5DS9X8 jd7aplD b8t4RM jHh8hq KtaPWF9cQj Q1hHN5WoXSiT MMxsVeq tdLUzvVTzj]\n",
      "        >>> print f\n",
      "        <type \"netCDF4._netCDF4.Dataset\">\n",
      "        root group (NETCDF4 file format):\n",
      "            dimensions: x, y, z\n",
      "            variables: phony_vlen_var, strvar\n",
      "            groups:\n",
      "        >>> print f.variables[\"strvar\"]\n",
      "        <type \"netCDF4._netCDF4.Variable\">\n",
      "        vlen strvar(z)\n",
      "        vlen data type: <type \"str\">\n",
      "        unlimited dimensions:\n",
      "        current size = (10,)\n",
      "    \n",
      "    It is also possible to set contents of vlen string variables with numpy arrays\n",
      "    of any string or unicode data type. Note, however, that accessing the contents\n",
      "    of such variables will always return numpy arrays with dtype `object`.\n",
      "    \n",
      "    ## <div id='section12'>12) Enum data type.\n",
      "    \n",
      "    netCDF4 has an enumerated data type, which is an integer datatype that is\n",
      "    restricted to certain named values. Since Enums don't map directly to\n",
      "    a numpy data type, they are read and written as integer arrays.\n",
      "    \n",
      "    Here's an example of using an Enum type to hold cloud type data.\n",
      "    The base integer data type and a python dictionary describing the allowed\n",
      "    values and their names are used to define an Enum data type using\n",
      "    `netCDF4.Dataset.createEnumType`.\n",
      "    \n",
      "        :::python\n",
      "        >>> nc = Dataset('clouds.nc','w')\n",
      "        >>> # python dict with allowed values and their names.\n",
      "        >>> enum_dict = {u'Altocumulus': 7, u'Missing': 255,\n",
      "        >>> u'Stratus': 2, u'Clear': 0,\n",
      "        >>> u'Nimbostratus': 6, u'Cumulus': 4, u'Altostratus': 5,\n",
      "        >>> u'Cumulonimbus': 1, u'Stratocumulus': 3}\n",
      "        >>> # create the Enum type called 'cloud_t'.\n",
      "        >>> cloud_type = nc.createEnumType(numpy.uint8,'cloud_t',enum_dict)\n",
      "        >>> print cloud_type\n",
      "        <type 'netCDF4._netCDF4.EnumType'>: name = 'cloud_t',\n",
      "        numpy dtype = uint8, fields/values ={u'Cumulus': 4,\n",
      "        u'Altocumulus': 7, u'Missing': 255,\n",
      "        u'Stratus': 2, u'Clear': 0,\n",
      "        u'Cumulonimbus': 1, u'Stratocumulus': 3,\n",
      "        u'Nimbostratus': 6, u'Altostratus': 5}\n",
      "    \n",
      "    A new variable can be created in the usual way using this data type.\n",
      "    Integer data is written to the variable that represents the named\n",
      "    cloud types in enum_dict. A `ValueError` will be raised if an attempt\n",
      "    is made to write an integer value not associated with one of the\n",
      "    specified names.\n",
      "    \n",
      "        :::python\n",
      "        >>> time = nc.createDimension('time',None)\n",
      "        >>> # create a 1d variable of type 'cloud_type'.\n",
      "        >>> # The fill_value is set to the 'Missing' named value.\n",
      "        >>> cloud_var =\n",
      "        >>> nc.createVariable('primary_cloud',cloud_type,'time',\n",
      "        >>> fill_value=enum_dict['Missing'])\n",
      "        >>> # write some data to the variable.\n",
      "        >>> cloud_var[:] = [enum_dict['Clear'],enum_dict['Stratus'],\n",
      "        >>> enum_dict['Cumulus'],enum_dict['Missing'],\n",
      "        >>> enum_dict['Cumulonimbus']]\n",
      "        >>> nc.close()\n",
      "        >>> # reopen the file, read the data.\n",
      "        >>> nc = Dataset('clouds.nc')\n",
      "        >>> cloud_var = nc.variables['primary_cloud']\n",
      "        >>> print cloud_var\n",
      "        <type 'netCDF4._netCDF4.Variable'>\n",
      "        enum primary_cloud(time)\n",
      "            _FillValue: 255\n",
      "        enum data type: uint8\n",
      "        unlimited dimensions: time\n",
      "        current shape = (5,)\n",
      "        >>> print cloud_var.datatype.enum_dict\n",
      "        {u'Altocumulus': 7, u'Missing': 255, u'Stratus': 2,\n",
      "        u'Clear': 0, u'Nimbostratus': 6, u'Cumulus': 4,\n",
      "        u'Altostratus': 5, u'Cumulonimbus': 1,\n",
      "        u'Stratocumulus': 3}\n",
      "        >>> print cloud_var[:]\n",
      "        [0 2 4 -- 1]\n",
      "        >>> nc.close()\n",
      "    \n",
      "    ## <div id='section13'>13) Parallel IO.\n",
      "    \n",
      "    If MPI parallel enabled versions of netcdf and hdf5 or pnetcdf are detected,\n",
      "    and [mpi4py](https://mpi4py.scipy.org) is installed, netcdf4-python will\n",
      "    be built with parallel IO capabilities enabled. Parallel IO of NETCDF4 or \n",
      "    NETCDF4_CLASSIC formatted files is only available if the MPI parallel HDF5\n",
      "    library is available. Parallel IO of classic netcdf-3 file formats is only\n",
      "    available if the [PnetCDF](https://parallel-netcdf.github.io/) library is\n",
      "    available. To use parallel IO, your program must be running in an MPI\n",
      "    environment using [mpi4py](https://mpi4py.scipy.org).\n",
      "    \n",
      "        :::python\n",
      "        >>> from mpi4py import MPI\n",
      "        >>> import numpy as np\n",
      "        >>> from netCDF4 import Dataset\n",
      "        >>> rank = MPI.COMM_WORLD.rank  # The process ID (integer 0-3 for 4-process run)\n",
      "    \n",
      "    To run an MPI-based parallel program like this, you must use `mpiexec` to launch several\n",
      "    parallel instances of Python (for example, using `mpiexec -np 4 python mpi_example.py`).\n",
      "    The parallel features of netcdf4-python are mostly transparent -\n",
      "    when a new dataset is created or an existing dataset is opened,\n",
      "    use the `parallel` keyword to enable parallel access.\n",
      "    \n",
      "        :::python\n",
      "        >>> nc = Dataset('parallel_tst.nc','w',parallel=True)\n",
      "    \n",
      "    The optional `comm` keyword may be used to specify a particular\n",
      "    MPI communicator (`MPI_COMM_WORLD` is used by default).  Each process (or rank)\n",
      "    can now write to the file indepedently.  In this example the process rank is\n",
      "    written to a different variable index on each task\n",
      "    \n",
      "        :::python\n",
      "        >>> d = nc.createDimension('dim',4)\n",
      "        >>> v = nc.createVariable('var', numpy.int, 'dim')\n",
      "        >>> v[rank] = rank\n",
      "        >>> nc.close()\n",
      "    \n",
      "        % ncdump parallel_test.nc\n",
      "        netcdf parallel_test {\n",
      "        dimensions:\n",
      "            dim = 4 ;\n",
      "            variables:\n",
      "            int64 var(dim) ;\n",
      "            data:\n",
      "    \n",
      "            var = 0, 1, 2, 3 ;\n",
      "        }\n",
      "    \n",
      "    There are two types of parallel IO, independent (the default) and collective.\n",
      "    Independent IO means that each process can do IO independently. It should not\n",
      "    depend on or be affected by other processes. Collective IO is a way of doing\n",
      "    IO defined in the MPI-IO standard; unlike independent IO, all processes must\n",
      "    participate in doing IO. To toggle back and forth between\n",
      "    the two types of IO, use the `netCDF4.Variable.set_collective`\n",
      "    `netCDF4.Variable`method. All metadata\n",
      "    operations (such as creation of groups, types, variables, dimensions, or attributes)\n",
      "    are collective.  There are a couple of important limitations of parallel IO:\n",
      "    \n",
      "     - parallel IO for NETCDF4 or NETCDF4_CLASSIC formatted files is only available\n",
      "       if the netcdf library was compiled with MPI enabled HDF5.\n",
      "     - parallel IO for all classic netcdf-3 file formats is only available if the\n",
      "       netcdf library was compiled with PnetCDF.\n",
      "     - If a variable has an unlimited dimension, appending data must be done in collective mode.\n",
      "       If the write is done in independent mode, the operation will fail with a\n",
      "       a generic \"HDF Error\".\n",
      "     - You cannot write compressed data in parallel (although\n",
      "       you can read it).\n",
      "     - You cannot use variable-length (VLEN) data types.\n",
      "    \n",
      "    ## <div id='section14'>14) Dealing with strings.\n",
      "    \n",
      "    The most flexible way to store arrays of strings is with the\n",
      "    [Variable-length (vlen) string data type](#section11). However, this requires\n",
      "    the use of the NETCDF4 data model, and the vlen type does not map very well\n",
      "    numpy arrays (you have to use numpy arrays of dtype=`object`, which are arrays of\n",
      "    arbitrary python objects). numpy does have a fixed-width string array\n",
      "    data type, but unfortunately the netCDF data model does not.\n",
      "    Instead fixed-width byte strings are typically stored as [arrays of 8-bit\n",
      "    characters](https://www.unidata.ucar.edu/software/netcdf/docs/BestPractices.html#bp_Strings-and-Variables-of-type-char).\n",
      "    To perform the conversion to and from character arrays to fixed-width numpy string arrays, the\n",
      "    following convention is followed by the python interface.\n",
      "    If the `_Encoding` special attribute is set for a character array\n",
      "    (dtype `S1`) variable, the `netCDF4.chartostring` utility function is used to convert the array of\n",
      "    characters to an array of strings with one less dimension (the last dimension is\n",
      "    interpreted as the length of each string) when reading the data. The character\n",
      "    set (usually ascii) is specified by the `_Encoding` attribute. If `_Encoding`\n",
      "    is 'none' or 'bytes', then the character array is converted to a numpy\n",
      "    fixed-width byte string array (dtype `S#`), otherwise a numpy unicode (dtype\n",
      "    `U#`) array is created.  When writing the data,\n",
      "    `netCDF4.stringtochar` is used to convert the numpy string array to an array of\n",
      "    characters with one more dimension. For example,\n",
      "    \n",
      "        :::python\n",
      "        >>> nc = Dataset('stringtest.nc','w',format='NETCDF4_CLASSIC')\n",
      "        >>> nc.createDimension('nchars',3)\n",
      "        >>> nc.createDimension('nstrings',None)\n",
      "        >>> v = nc.createVariable('strings','S1',('nstrings','nchars'))\n",
      "        >>> datain = numpy.array(['foo','bar'],dtype='S3')\n",
      "        >>> v[:] = stringtochar(datain) # manual conversion to char array\n",
      "        >>> v[:] # data returned as char array\n",
      "        [[b'f' b'o' b'o']\n",
      "        [b'b' b'a' b'r']]\n",
      "        >>> v._Encoding = 'ascii' # this enables automatic conversion\n",
      "        >>> v[:] = datain # conversion to char array done internally\n",
      "        >>> v[:] # data returned in numpy string array\n",
      "        ['foo' 'bar']\n",
      "        >>> nc.close()\n",
      "    \n",
      "    Even if the `_Encoding` attribute is set, the automatic conversion of char\n",
      "    arrays to/from string arrays can be disabled with\n",
      "    `netCDF4.Variable.set_auto_chartostring`.\n",
      "    \n",
      "    A similar situation is often encountered with numpy structured arrays with subdtypes\n",
      "    containing fixed-wdith byte strings (dtype=`S#`). Since there is no native fixed-length string\n",
      "    netCDF datatype, these numpy structure arrays are mapped onto netCDF compound\n",
      "    types with character array elements.  In this case the string <-> char array\n",
      "    conversion is handled automatically (without the need to set the `_Encoding`\n",
      "    attribute) using [numpy\n",
      "    views](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.view.html).\n",
      "    The structured array dtype (including the string elements) can even be used to\n",
      "    define the compound data type - the string dtype will be converted to\n",
      "    character array dtype under the hood when creating the netcdf compound type.\n",
      "    Here's an example:\n",
      "    \n",
      "        :::python\n",
      "        >>> nc = Dataset('compoundstring_example.nc','w')\n",
      "        >>> dtype = numpy.dtype([('observation', 'f4'),\n",
      "                          ('station_name','S80')])\n",
      "        >>> station_data_t = nc.createCompoundType(dtype,'station_data')\n",
      "        >>> nc.createDimension('station',None)\n",
      "        >>> statdat = nc.createVariable('station_obs', station_data_t, ('station',))\n",
      "        >>> data = numpy.empty(2,dtype)\n",
      "        >>> data['observation'][:] = (123.,3.14)\n",
      "        >>> data['station_name'][:] = ('Boulder','New York')\n",
      "        >>> statdat.dtype # strings actually stored as character arrays\n",
      "        {'names':['observation','station_name'], 'formats':['<f4',('S1', (80,))], 'offsets':[0,4], 'itemsize':84, 'aligned':True}\n",
      "        >>> statdat[:] = data # strings converted to character arrays internally\n",
      "        >>> statdat[:] # character arrays converted back to strings\n",
      "        [(123.  , 'Boulder') (  3.14, 'New York')]\n",
      "        >>> statdat[:].dtype\n",
      "        {'names':['observation','station_name'], 'formats':['<f4','S80'], 'offsets':[0,4], 'itemsize':84, 'aligned':True}\n",
      "        >>> statdat.set_auto_chartostring(False) # turn off auto-conversion\n",
      "        >>> statdat[:] = data.view(dtype=[('observation', 'f4'),('station_name','S1',10)])\n",
      "        >>> statdat[:] # now structured array with char array subtype is returned\n",
      "        [(123.  , ['B', 'o', 'u', 'l', 'd', 'e', 'r', '', '', ''])\n",
      "        (  3.14, ['N', 'e', 'w', ' ', 'Y', 'o', 'r', 'k', '', ''])]\n",
      "        >>> nc.close()\n",
      "    \n",
      "    Note that there is currently no support for mapping numpy structured arrays with\n",
      "    unicode elements (dtype `U#`) onto netCDF compound types, nor is there support\n",
      "    for netCDF compound types with vlen string components.\n",
      "    \n",
      "    ## <div id='section15'>15) In-memory (diskless) Datasets.\n",
      "    \n",
      "    You can create netCDF Datasets whose content is held in memory\n",
      "    instead of in a disk file.  There are two ways to do this.  If you\n",
      "    don't need access to the memory buffer containing the Dataset from\n",
      "    within python, the best way is to use the `diskless=True` keyword\n",
      "    argument when creating the Dataset.  If you want to save the Dataset\n",
      "    to disk when you close it, also set `persist=True`.  If you want to\n",
      "    create a new read-only Dataset from an existing python memory buffer, use the\n",
      "    `memory` keyword argument to pass the memory buffer when creating the Dataset.\n",
      "    If you want to create a new in-memory Dataset, and then access the memory buffer\n",
      "    directly from Python, use the `memory` keyword argument to specify the\n",
      "    estimated size of the Dataset in bytes when creating the Dataset with\n",
      "    `mode='w'`.  Then, the `Dataset.close` method will return a python memoryview\n",
      "    object representing the Dataset. Below are examples illustrating both\n",
      "    approaches.\n",
      "    \n",
      "        :::python\n",
      "        >>> # create a diskless (in-memory) Dataset,\n",
      "        >>> # and persist the file to disk when it is closed.\n",
      "        >>> nc = Dataset('diskless_example.nc','w',diskless=True,persist=True)\n",
      "        >>> d = nc.createDimension('x',None)\n",
      "        >>> v = nc.createVariable('v',numpy.int32,'x')\n",
      "        >>> v[0:5] = numpy.arange(5)\n",
      "        >>> print(nc)\n",
      "        <type 'netCDF4._netCDF4.Dataset'>\n",
      "        root group (NETCDF4 data model, file format HDF5):\n",
      "        dimensions(sizes): x(5)\n",
      "        variables(dimensions): int32 v(x)\n",
      "        groups:\n",
      "        >>> print(nc['v'][:])\n",
      "        [0 1 2 3 4]\n",
      "        >>> nc.close() # file saved to disk\n",
      "        >>> # create an in-memory dataset from an existing python\n",
      "        >>> # python memory buffer.\n",
      "        >>> # read the newly created netcdf file into a python\n",
      "        >>> # bytes object.\n",
      "        >>> f = open('diskless_example.nc', 'rb')\n",
      "        >>> nc_bytes = f.read(); f.close()\n",
      "        >>> # create a netCDF in-memory dataset from the bytes object.\n",
      "        >>> nc = Dataset('inmemory.nc', memory=nc_bytes)\n",
      "        >>> print(nc)\n",
      "        <type 'netCDF4._netCDF4.Dataset'>\n",
      "        root group (NETCDF4 data model, file format HDF5):\n",
      "        dimensions(sizes): x(5)\n",
      "        variables(dimensions): int32 v(x)\n",
      "        groups:\n",
      "        >>> print(nc['v'][:])\n",
      "        [0 1 2 3 4]\n",
      "        >>> nc.close()\n",
      "        >>> # create an in-memory Dataset and retrieve memory buffer\n",
      "        >>> # estimated size is 1028 bytes - this is actually only\n",
      "        >>> # used if format is NETCDF3\n",
      "        >>> # (ignored for NETCDF4/HDF5 files).\n",
      "        >>> nc = Dataset('inmemory.nc', mode='w',memory=1028)\n",
      "        >>> d = nc.createDimension('x',None)\n",
      "        >>> v = nc.createVariable('v',numpy.int32,'x')\n",
      "        >>> v[0:5] = numpy.arange(5)\n",
      "        >>> nc_buf = nc.close() # close returns memoryview\n",
      "        >>> print(type(nc_buf))\n",
      "        <type 'memoryview'>\n",
      "        >>> # save nc_buf to disk, read it back in and check.\n",
      "        >>> f = open('inmemory.nc', 'wb')\n",
      "        >>> f.write(nc_buf); f.close()\n",
      "        >>> nc = Dataset('inmemory.nc')\n",
      "        >>> print(nc)\n",
      "        <type 'netCDF4._netCDF4.Dataset'>\n",
      "        root group (NETCDF4 data model, file format HDF5):\n",
      "        dimensions(sizes): x(5)\n",
      "        variables(dimensions): int32 v(x)\n",
      "        groups:\n",
      "        >>> print(nc['v'][:])\n",
      "        [0 1 2 3 4]\n",
      "        >>> nc.close()\n",
      "    \n",
      "    \n",
      "    All of the code in this tutorial is available in `examples/tutorial.py`, except\n",
      "    the parallel IO example, which is in `examples/mpi_example.py`.\n",
      "    Unit tests are in the `test` directory.\n",
      "    \n",
      "    **contact**: Jeffrey Whitaker <jeffrey.s.whitaker@noaa.gov>\n",
      "    \n",
      "    **copyright**: 2008 by Jeffrey Whitaker.\n",
      "    \n",
      "    **license**: Permission to use, copy, modify, and distribute this software and\n",
      "    its documentation for any purpose and without fee is hereby granted,\n",
      "    provided that the above copyright notice appear in all copies and that\n",
      "    both the copyright notice and this permission notice appear in\n",
      "    supporting documentation.\n",
      "    THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,\n",
      "    INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO\n",
      "    EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR\n",
      "    CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF\n",
      "    USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR\n",
      "    OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR\n",
      "    PERFORMANCE OF THIS SOFTWARE.\n",
      "    - - -\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _netCDF4\n",
      "    utils\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        netCDF4._netCDF4.CompoundType\n",
      "        netCDF4._netCDF4.Dataset\n",
      "            netCDF4._netCDF4.Group\n",
      "            netCDF4._netCDF4.MFDataset\n",
      "        netCDF4._netCDF4.Dimension\n",
      "        netCDF4._netCDF4.EnumType\n",
      "        netCDF4._netCDF4.VLType\n",
      "        netCDF4._netCDF4.Variable\n",
      "    netCDF4._netCDF4._Variable(builtins.object)\n",
      "        netCDF4._netCDF4.MFTime\n",
      "    \n",
      "    class CompoundType(builtins.object)\n",
      "     |  A `netCDF4.CompoundType` instance is used to describe a compound data\n",
      "     |  type, and can be passed to the the `netCDF4.Dataset.createVariable` method of\n",
      "     |  a `netCDF4.Dataset` or `netCDF4.Group` instance.\n",
      "     |  Compound data types map to numpy structured arrays.\n",
      "     |  See `netCDF4.CompoundType.__init__` for more details.\n",
      "     |  \n",
      "     |  The instance variables `dtype` and `name` should not be modified by\n",
      "     |  the user.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      ***`__init__(group, datatype, datatype_name)`***\n",
      "     |      \n",
      "     |      CompoundType constructor.\n",
      "     |      \n",
      "     |      **`group`**: `netCDF4.Group` instance to associate with the compound datatype.\n",
      "     |      \n",
      "     |      **`datatype`**: A numpy dtype object describing a structured (a.k.a record)\n",
      "     |      array.  Can be composed of homogeneous numeric or character data types, or\n",
      "     |      other structured array data types.\n",
      "     |      \n",
      "     |      **`datatype_name`**: a Python string containing a description of the\n",
      "     |      compound data type.\n",
      "     |      \n",
      "     |      ***Note 1***: When creating nested compound data types,\n",
      "     |      the inner compound data types must already be associated with CompoundType\n",
      "     |      instances (so create CompoundType instances for the innermost structures\n",
      "     |      first).\n",
      "     |      \n",
      "     |      ***Note 2***: `netCDF4.CompoundType` instances should be created using the\n",
      "     |      `netCDF4.Dataset.createCompoundType`\n",
      "     |      method of a `netCDF4.Dataset` or `netCDF4.Group` instance, not using this class directly.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  dtype_view\n",
      "     |  \n",
      "     |  name\n",
      "    \n",
      "    class Dataset(builtins.object)\n",
      "     |  A netCDF `netCDF4.Dataset` is a collection of dimensions, groups, variables and\n",
      "     |  attributes. Together they describe the meaning of data and relations among\n",
      "     |  data fields stored in a netCDF file. See `netCDF4.Dataset.__init__` for more\n",
      "     |  details.\n",
      "     |  \n",
      "     |  A list of attribute names corresponding to global netCDF attributes\n",
      "     |  defined for the `netCDF4.Dataset` can be obtained with the\n",
      "     |  `netCDF4.Dataset.ncattrs` method.\n",
      "     |  These attributes can be created by assigning to an attribute of the\n",
      "     |  `netCDF4.Dataset` instance. A dictionary containing all the netCDF attribute\n",
      "     |  name/value pairs is provided by the `__dict__` attribute of a\n",
      "     |  `netCDF4.Dataset` instance.\n",
      "     |  \n",
      "     |  The following class variables are read-only and should not be\n",
      "     |  modified by the user.\n",
      "     |  \n",
      "     |  **`dimensions`**: The `dimensions` dictionary maps the names of\n",
      "     |  dimensions defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances of the\n",
      "     |  `netCDF4.Dimension` class.\n",
      "     |  \n",
      "     |  **`variables`**: The `variables` dictionary maps the names of variables\n",
      "     |  defined for this `netCDF4.Dataset` or `netCDF4.Group` to instances of the\n",
      "     |  `netCDF4.Variable` class.\n",
      "     |  \n",
      "     |  **`groups`**: The groups dictionary maps the names of groups created for\n",
      "     |  this `netCDF4.Dataset` or `netCDF4.Group` to instances of the `netCDF4.Group` class (the\n",
      "     |  `netCDF4.Dataset` class is simply a special case of the `netCDF4.Group` class which\n",
      "     |  describes the root group in the netCDF4 file).\n",
      "     |  \n",
      "     |  **`cmptypes`**: The `cmptypes` dictionary maps the names of\n",
      "     |  compound types defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances of the\n",
      "     |  `netCDF4.CompoundType` class.\n",
      "     |  \n",
      "     |  **`vltypes`**: The `vltypes` dictionary maps the names of\n",
      "     |  variable-length types defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances\n",
      "     |  of the `netCDF4.VLType` class.\n",
      "     |  \n",
      "     |  **`enumtypes`**: The `enumtypes` dictionary maps the names of\n",
      "     |  Enum types defined for the `netCDF4.Group` or `netCDF4.Dataset` to instances\n",
      "     |  of the `netCDF4.EnumType` class.\n",
      "     |  \n",
      "     |  **`data_model`**: `data_model` describes the netCDF\n",
      "     |  data model version, one of `NETCDF3_CLASSIC`, `NETCDF4`,\n",
      "     |  `NETCDF4_CLASSIC`, `NETCDF3_64BIT_OFFSET` or `NETCDF3_64BIT_DATA`.\n",
      "     |  \n",
      "     |  **`file_format`**: same as `data_model`, retained for backwards compatibility.\n",
      "     |  \n",
      "     |  **`disk_format`**: `disk_format` describes the underlying\n",
      "     |  file format, one of `NETCDF3`, `HDF5`, `HDF4`,\n",
      "     |  `PNETCDF`, `DAP2`, `DAP4` or `UNDEFINED`. Only available if using\n",
      "     |  netcdf C library version >= 4.3.1, otherwise will always return\n",
      "     |  `UNDEFINED`.\n",
      "     |  \n",
      "     |  **`parent`**: `parent` is a reference to the parent\n",
      "     |  `netCDF4.Group` instance. `None` for the root group or `netCDF4.Dataset`\n",
      "     |  instance.\n",
      "     |  \n",
      "     |  **`path`**: `path` shows the location of the `netCDF4.Group` in\n",
      "     |  the `netCDF4.Dataset` in a unix directory format (the names of groups in the\n",
      "     |  hierarchy separated by backslashes). A `netCDF4.Dataset` instance is the root\n",
      "     |  group, so the path is simply `'/'`.\n",
      "     |  \n",
      "     |  **`keepweakref`**: If `True`, child Dimension and Variables objects only keep weak\n",
      "     |  references to the parent Dataset or Group.\n",
      "     |  \n",
      "     |  **`_ncstring_attrs__`**: If `True`, all text attributes will be written as variable-length\n",
      "     |  strings.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __enter__(...)\n",
      "     |  \n",
      "     |  __exit__(...)\n",
      "     |  \n",
      "     |  __getattr__(...)\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      **`__init__(self, filename, mode=\"r\", clobber=True, diskless=False,\n",
      "     |      persist=False, keepweakref=False, memory=None, encoding=None,\n",
      "     |      parallel=False, comm=None, info=None, format='NETCDF4')`**\n",
      "     |      \n",
      "     |      `netCDF4.Dataset` constructor.\n",
      "     |      \n",
      "     |      **`filename`**: Name of netCDF file to hold dataset. Can also\n",
      "     |      be a python 3 pathlib instance or the URL of an OpenDAP dataset.  When memory is\n",
      "     |      set this is just used to set the `filepath()`.\n",
      "     |      \n",
      "     |      **`mode`**: access mode. `r` means read-only; no data can be\n",
      "     |      modified. `w` means write; a new file is created, an existing file with\n",
      "     |      the same name is deleted. `a` and `r+` mean append (in analogy with\n",
      "     |      serial files); an existing file is opened for reading and writing.\n",
      "     |      Appending `s` to modes `r`, `w`, `r+` or `a` will enable unbuffered shared\n",
      "     |      access to `NETCDF3_CLASSIC`, `NETCDF3_64BIT_OFFSET` or\n",
      "     |      `NETCDF3_64BIT_DATA` formatted files.\n",
      "     |      Unbuffered access may be useful even if you don't need shared\n",
      "     |      access, since it may be faster for programs that don't access data\n",
      "     |      sequentially. This option is ignored for `NETCDF4` and `NETCDF4_CLASSIC`\n",
      "     |      formatted files.\n",
      "     |      \n",
      "     |      **`clobber`**: if `True` (default), opening a file with `mode='w'`\n",
      "     |      will clobber an existing file with the same name.  if `False`, an\n",
      "     |      exception will be raised if a file with the same name already exists.\n",
      "     |      \n",
      "     |      **`format`**: underlying file format (one of `'NETCDF4',\n",
      "     |      'NETCDF4_CLASSIC', 'NETCDF3_CLASSIC'`, `'NETCDF3_64BIT_OFFSET'` or\n",
      "     |      `'NETCDF3_64BIT_DATA'`.\n",
      "     |      Only relevant if `mode = 'w'` (if `mode = 'r','a'` or `'r+'` the file format\n",
      "     |      is automatically detected). Default `'NETCDF4'`, which means the data is\n",
      "     |      stored in an HDF5 file, using netCDF 4 API features.  Setting\n",
      "     |      `format='NETCDF4_CLASSIC'` will create an HDF5 file, using only netCDF 3\n",
      "     |      compatible API features. netCDF 3 clients must be recompiled and linked\n",
      "     |      against the netCDF 4 library to read files in `NETCDF4_CLASSIC` format.\n",
      "     |      `'NETCDF3_CLASSIC'` is the classic netCDF 3 file format that does not\n",
      "     |      handle 2+ Gb files. `'NETCDF3_64BIT_OFFSET'` is the 64-bit offset\n",
      "     |      version of the netCDF 3 file format, which fully supports 2+ GB files, but\n",
      "     |      is only compatible with clients linked against netCDF version 3.6.0 or\n",
      "     |      later. `'NETCDF3_64BIT_DATA'` is the 64-bit data version of the netCDF 3\n",
      "     |      file format, which supports 64-bit dimension sizes plus unsigned and\n",
      "     |      64 bit integer data types, but is only compatible with clients linked against\n",
      "     |      netCDF version 4.4.0 or later.\n",
      "     |      \n",
      "     |      **`diskless`**: If `True`, create diskless (in-core) file.\n",
      "     |      This is a feature added to the C library after the\n",
      "     |      netcdf-4.2 release. If you need to access the memory buffer directly,\n",
      "     |      use the in-memory feature instead (see `memory` kwarg).\n",
      "     |      \n",
      "     |      **`persist`**: if `diskless=True`, persist file to disk when closed\n",
      "     |      (default `False`).\n",
      "     |      \n",
      "     |      **`keepweakref`**: if `True`, child Dimension and Variable instances will keep weak\n",
      "     |      references to the parent Dataset or Group object.  Default is `False`, which\n",
      "     |      means strong references will be kept.  Having Dimension and Variable instances\n",
      "     |      keep a strong reference to the parent Dataset instance, which in turn keeps a\n",
      "     |      reference to child Dimension and Variable instances, creates circular references.\n",
      "     |      Circular references complicate garbage collection, which may mean increased\n",
      "     |      memory usage for programs that create may Dataset instances with lots of\n",
      "     |      Variables. It also will result in the Dataset object never being deleted, which\n",
      "     |      means it may keep open files alive as well. Setting `keepweakref=True` allows\n",
      "     |      Dataset instances to be garbage collected as soon as they go out of scope, potentially\n",
      "     |      reducing memory usage and open file handles.  However, in many cases this is not\n",
      "     |      desirable, since the associated Variable instances may still be needed, but are\n",
      "     |      rendered unusable when the parent Dataset instance is garbage collected.\n",
      "     |      \n",
      "     |      **`_ncstring_attrs__`**: if `_ncstring_attrs__=True`, all string attributes will use\n",
      "     |      the variable length NC_STRING attributes (default `False`, ascii text\n",
      "     |      attributes written as NC_CHAR).\n",
      "     |      \n",
      "     |      **`memory`**: if not `None`, create or open an in-memory Dataset.\n",
      "     |      If mode = 'r', the memory kwarg must contain a memory buffer object\n",
      "     |      (an object that supports the python buffer interface).\n",
      "     |      The Dataset will then be created with contents taken from this block of memory.\n",
      "     |      If mode = 'w', the memory kwarg should contain the anticipated size\n",
      "     |      of the Dataset in bytes (used only for NETCDF3 files).  A memory\n",
      "     |      buffer containing a copy of the Dataset is returned by the\n",
      "     |      `Dataset.close` method. Requires netcdf-c version 4.4.1 for mode='r,\n",
      "     |      netcdf-c 4.6.2 for mode='w'. To persist the file to disk, the raw\n",
      "     |      bytes from the returned buffer can be written into a binary file.\n",
      "     |      The Dataset can also be re-opened using this memory buffer.\n",
      "     |      \n",
      "     |      **`encoding`**: encoding used to encode filename string into bytes.\n",
      "     |      Default is None (`sys.getdefaultfileencoding()` is used).\n",
      "     |      \n",
      "     |      **`parallel`**: open for parallel access using MPI (requires mpi4py and\n",
      "     |      parallel-enabled netcdf-c and hdf5 libraries).  Default is `False`. If\n",
      "     |      `True`, `comm` and `info` kwargs may also be specified.\n",
      "     |      \n",
      "     |      **`comm`**: MPI_Comm object for parallel access. Default `None`, which\n",
      "     |      means MPI_COMM_WORLD will be used.  Ignored if `parallel=False`.\n",
      "     |      \n",
      "     |      **`info`**: MPI_Info object for parallel access. Default `None`, which\n",
      "     |      means MPI_INFO_NULL will be used.  Ignored if `parallel=False`.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  close(...)\n",
      "     |      **`close(self)`**\n",
      "     |      \n",
      "     |      Close the Dataset.\n",
      "     |  \n",
      "     |  createCompoundType(...)\n",
      "     |      **`createCompoundType(self, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      Creates a new compound data type named `datatype_name` from the numpy\n",
      "     |      dtype object `datatype`.\n",
      "     |      \n",
      "     |      ***Note***: If the new compound data type contains other compound data types\n",
      "     |      (i.e. it is a 'nested' compound type, where not all of the elements\n",
      "     |      are homogeneous numeric data types), then the 'inner' compound types **must** be\n",
      "     |      created first.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.CompoundType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createDimension(...)\n",
      "     |      **`createDimension(self, dimname, size=None)`**\n",
      "     |      \n",
      "     |      Creates a new dimension with the given `dimname` and `size`.\n",
      "     |      \n",
      "     |      `size` must be a positive integer or `None`, which stands for\n",
      "     |      \"unlimited\" (default is `None`). Specifying a size of 0 also\n",
      "     |      results in an unlimited dimension. The return value is the `netCDF4.Dimension`\n",
      "     |      class instance describing the new dimension.  To determine the current\n",
      "     |      maximum size of the dimension, use the `len` function on the `netCDF4.Dimension`\n",
      "     |      instance. To determine if a dimension is 'unlimited', use the\n",
      "     |      `netCDF4.Dimension.isunlimited` method of the `netCDF4.Dimension` instance.\n",
      "     |  \n",
      "     |  createEnumType(...)\n",
      "     |      **`createEnumType(self, datatype, datatype_name, enum_dict)`**\n",
      "     |      \n",
      "     |      Creates a new Enum data type named `datatype_name` from a numpy\n",
      "     |      integer dtype object `datatype`, and a python dictionary\n",
      "     |      defining the enum fields and values.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.EnumType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createGroup(...)\n",
      "     |      **`createGroup(self, groupname)`**\n",
      "     |      \n",
      "     |      Creates a new `netCDF4.Group` with the given `groupname`.\n",
      "     |      \n",
      "     |      If `groupname` is specified as a path, using forward slashes as in unix to\n",
      "     |      separate components, then intermediate groups will be created as necessary\n",
      "     |      (analogous to `mkdir -p` in unix).  For example,\n",
      "     |      `createGroup('/GroupA/GroupB/GroupC')` will create `GroupA`,\n",
      "     |      `GroupA/GroupB`, and `GroupA/GroupB/GroupC`, if they don't already exist.\n",
      "     |      If the specified path describes a group that already exists, no error is\n",
      "     |      raised.\n",
      "     |      \n",
      "     |      The return value is a `netCDF4.Group` class instance.\n",
      "     |  \n",
      "     |  createVLType(...)\n",
      "     |      **`createVLType(self, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      Creates a new VLEN data type named `datatype_name` from a numpy\n",
      "     |      dtype object `datatype`.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.VLType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createVariable(...)\n",
      "     |      **`createVariable(self, varname, datatype, dimensions=(), zlib=False,\n",
      "     |      complevel=4, shuffle=True, fletcher32=False, contiguous=False, chunksizes=None,\n",
      "     |      endian='native', least_significant_digit=None, fill_value=None)`**\n",
      "     |      \n",
      "     |      Creates a new variable with the given `varname`, `datatype`, and\n",
      "     |      `dimensions`. If dimensions are not given, the variable is assumed to be\n",
      "     |      a scalar.\n",
      "     |      \n",
      "     |      If `varname` is specified as a path, using forward slashes as in unix to\n",
      "     |      separate components, then intermediate groups will be created as necessary\n",
      "     |      For example, `createVariable('/GroupA/GroupB/VarC', float, ('x','y'))` will create groups `GroupA`\n",
      "     |      and `GroupA/GroupB`, plus the variable `GroupA/GroupB/VarC`, if the preceding\n",
      "     |      groups don't already exist.\n",
      "     |      \n",
      "     |      The `datatype` can be a numpy datatype object, or a string that describes\n",
      "     |      a numpy dtype object (like the `dtype.str` attribute of a numpy array).\n",
      "     |      Supported specifiers include: `'S1' or 'c' (NC_CHAR), 'i1' or 'b' or 'B'\n",
      "     |      (NC_BYTE), 'u1' (NC_UBYTE), 'i2' or 'h' or 's' (NC_SHORT), 'u2'\n",
      "     |      (NC_USHORT), 'i4' or 'i' or 'l' (NC_INT), 'u4' (NC_UINT), 'i8' (NC_INT64),\n",
      "     |      'u8' (NC_UINT64), 'f4' or 'f' (NC_FLOAT), 'f8' or 'd' (NC_DOUBLE)`.\n",
      "     |      `datatype` can also be a `netCDF4.CompoundType` instance\n",
      "     |      (for a structured, or compound array), a `netCDF4.VLType` instance\n",
      "     |      (for a variable-length array), or the python `str` builtin\n",
      "     |      (for a variable-length string array). Numpy string and unicode datatypes with\n",
      "     |      length greater than one are aliases for `str`.\n",
      "     |      \n",
      "     |      Data from netCDF variables is presented to python as numpy arrays with\n",
      "     |      the corresponding data type.\n",
      "     |      \n",
      "     |      `dimensions` must be a tuple containing dimension names (strings) that\n",
      "     |      have been defined previously using `netCDF4.Dataset.createDimension`. The default value\n",
      "     |      is an empty tuple, which means the variable is a scalar.\n",
      "     |      \n",
      "     |      If the optional keyword `zlib` is `True`, the data will be compressed in\n",
      "     |      the netCDF file using gzip compression (default `False`).\n",
      "     |      \n",
      "     |      The optional keyword `complevel` is an integer between 1 and 9 describing\n",
      "     |      the level of compression desired (default 4). Ignored if `zlib=False`.\n",
      "     |      \n",
      "     |      If the optional keyword `shuffle` is `True`, the HDF5 shuffle filter\n",
      "     |      will be applied before compressing the data (default `True`).  This\n",
      "     |      significantly improves compression. Default is `True`. Ignored if\n",
      "     |      `zlib=False`.\n",
      "     |      \n",
      "     |      If the optional keyword `fletcher32` is `True`, the Fletcher32 HDF5\n",
      "     |      checksum algorithm is activated to detect errors. Default `False`.\n",
      "     |      \n",
      "     |      If the optional keyword `contiguous` is `True`, the variable data is\n",
      "     |      stored contiguously on disk.  Default `False`. Setting to `True` for\n",
      "     |      a variable with an unlimited dimension will trigger an error.\n",
      "     |      \n",
      "     |      The optional keyword `chunksizes` can be used to manually specify the\n",
      "     |      HDF5 chunksizes for each dimension of the variable. A detailed\n",
      "     |      discussion of HDF chunking and I/O performance is available\n",
      "     |      [here](http://www.hdfgroup.org/HDF5/doc/H5.user/Chunking.html).\n",
      "     |      Basically, you want the chunk size for each dimension to match as\n",
      "     |      closely as possible the size of the data block that users will read\n",
      "     |      from the file.  `chunksizes` cannot be set if `contiguous=True`.\n",
      "     |      \n",
      "     |      The optional keyword `endian` can be used to control whether the\n",
      "     |      data is stored in little or big endian format on disk. Possible\n",
      "     |      values are `little, big` or `native` (default). The library\n",
      "     |      will automatically handle endian conversions when the data is read,\n",
      "     |      but if the data is always going to be read on a computer with the\n",
      "     |      opposite format as the one used to create the file, there may be\n",
      "     |      some performance advantage to be gained by setting the endian-ness.\n",
      "     |      \n",
      "     |      The `zlib, complevel, shuffle, fletcher32, contiguous, chunksizes` and `endian`\n",
      "     |      keywords are silently ignored for netCDF 3 files that do not use HDF5.\n",
      "     |      \n",
      "     |      The optional keyword `fill_value` can be used to override the default\n",
      "     |      netCDF `_FillValue` (the value that the variable gets filled with before\n",
      "     |      any data is written to it, defaults given in `netCDF4.default_fillvals`).\n",
      "     |      If fill_value is set to `False`, then the variable is not pre-filled.\n",
      "     |      \n",
      "     |      If the optional keyword parameter `least_significant_digit` is\n",
      "     |      specified, variable data will be truncated (quantized). In conjunction\n",
      "     |      with `zlib=True` this produces 'lossy', but significantly more\n",
      "     |      efficient compression. For example, if `least_significant_digit=1`,\n",
      "     |      data will be quantized using `numpy.around(scale*data)/scale`, where\n",
      "     |      scale = 2**bits, and bits is determined so that a precision of 0.1 is\n",
      "     |      retained (in this case bits=4). From the\n",
      "     |      [PSD metadata conventions](http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml):\n",
      "     |      \"least_significant_digit -- power of ten of the smallest decimal place\n",
      "     |      in unpacked data that is a reliable value.\" Default is `None`, or no\n",
      "     |      quantization, or 'lossless' compression.\n",
      "     |      \n",
      "     |      When creating variables in a `NETCDF4` or `NETCDF4_CLASSIC` formatted file,\n",
      "     |      HDF5 creates something called a 'chunk cache' for each variable.  The\n",
      "     |      default size of the chunk cache may be large enough to completely fill\n",
      "     |      available memory when creating thousands of variables.  The optional\n",
      "     |      keyword `chunk_cache` allows you to reduce (or increase) the size of\n",
      "     |      the default chunk cache when creating a variable.  The setting only\n",
      "     |      persists as long as the Dataset is open - you can use the set_var_chunk_cache\n",
      "     |      method to change it the next time the Dataset is opened.\n",
      "     |      Warning - messing with this parameter can seriously degrade performance.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.Variable` class instance describing the new\n",
      "     |      variable.\n",
      "     |      \n",
      "     |      A list of names corresponding to netCDF variable attributes can be\n",
      "     |      obtained with the `netCDF4.Variable` method `netCDF4.Variable.ncattrs`. A dictionary\n",
      "     |      containing all the netCDF attribute name/value pairs is provided by\n",
      "     |      the `__dict__` attribute of a `netCDF4.Variable` instance.\n",
      "     |      \n",
      "     |      `netCDF4.Variable` instances behave much like array objects. Data can be\n",
      "     |      assigned to or retrieved from a variable with indexing and slicing\n",
      "     |      operations on the `netCDF4.Variable` instance. A `netCDF4.Variable` instance has six\n",
      "     |      Dataset standard attributes: `dimensions, dtype, shape, ndim, name` and\n",
      "     |      `least_significant_digit`. Application programs should never modify\n",
      "     |      these attributes. The `dimensions` attribute is a tuple containing the\n",
      "     |      names of the dimensions associated with this variable. The `dtype`\n",
      "     |      attribute is a string describing the variable's data type (`i4, f8,\n",
      "     |      S1,` etc). The `shape` attribute is a tuple describing the current\n",
      "     |      sizes of all the variable's dimensions. The `name` attribute is a\n",
      "     |      string containing the name of the Variable instance.\n",
      "     |      The `least_significant_digit`\n",
      "     |      attributes describes the power of ten of the smallest decimal place in\n",
      "     |      the data the contains a reliable value.  assigned to the `netCDF4.Variable`\n",
      "     |      instance. If `None`, the data is not truncated. The `ndim` attribute\n",
      "     |      is the number of variable dimensions.\n",
      "     |  \n",
      "     |  delncattr(...)\n",
      "     |      **`delncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      delete a netCDF dataset or group attribute.  Use if you need to delete a\n",
      "     |      netCDF attribute with the same name as one of the reserved python\n",
      "     |      attributes.\n",
      "     |  \n",
      "     |  filepath(...)\n",
      "     |      **`filepath(self,encoding=None)`**\n",
      "     |      \n",
      "     |      Get the file system path (or the opendap URL) which was used to\n",
      "     |      open/create the Dataset. Requires netcdf >= 4.1.2.  The path\n",
      "     |      is decoded into a string using `sys.getfilesystemencoding()` by default, this can be\n",
      "     |      changed using the `encoding` kwarg.\n",
      "     |  \n",
      "     |  get_variables_by_attributes(...)\n",
      "     |      **`get_variables_by_attribute(self, **kwargs)`**\n",
      "     |      \n",
      "     |      Returns a list of variables that match specific conditions.\n",
      "     |      \n",
      "     |      Can pass in key=value parameters and variables are returned that\n",
      "     |      contain all of the matches. For example,\n",
      "     |      \n",
      "     |          :::python\n",
      "     |          >>> # Get variables with x-axis attribute.\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis='X')\n",
      "     |          >>> # Get variables with matching \"standard_name\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n",
      "     |      \n",
      "     |      Can pass in key=callable parameter and variables are returned if the\n",
      "     |      callable returns True.  The callable should accept a single parameter,\n",
      "     |      the attribute value.  None is given as the attribute value when the\n",
      "     |      attribute does not exist on the variable. For example,\n",
      "     |      \n",
      "     |          :::python\n",
      "     |          >>> # Get Axis variables\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis=lambda v: v in ['X', 'Y', 'Z', 'T'])\n",
      "     |          >>> # Get variables that don't have an \"axis\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis=lambda v: v is None)\n",
      "     |          >>> # Get variables that have a \"grid_mapping\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(grid_mapping=lambda v: v is not None)\n",
      "     |  \n",
      "     |  getncattr(...)\n",
      "     |      **`getncattr(self,name)`**\n",
      "     |      \n",
      "     |      retrieve a netCDF dataset or group attribute.\n",
      "     |      Use if you need to get a netCDF attribute with the same\n",
      "     |      name as one of the reserved python attributes.\n",
      "     |      \n",
      "     |      option kwarg `encoding` can be used to specify the\n",
      "     |      character encoding of a string attribute (default is `utf-8`).\n",
      "     |  \n",
      "     |  isopen(...)\n",
      "     |      **`close(self)`**\n",
      "     |      \n",
      "     |      is the Dataset open or closed?\n",
      "     |  \n",
      "     |  ncattrs(...)\n",
      "     |      **`ncattrs(self)`**\n",
      "     |      \n",
      "     |      return netCDF global attribute names for this `netCDF4.Dataset` or `netCDF4.Group` in a list.\n",
      "     |  \n",
      "     |  renameAttribute(...)\n",
      "     |      **`renameAttribute(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Dataset` or `netCDF4.Group` attribute named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  renameDimension(...)\n",
      "     |      **`renameDimension(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Dimension` named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  renameGroup(...)\n",
      "     |      **`renameGroup(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Group` named `oldname` to `newname` (requires netcdf >= 4.3.1).\n",
      "     |  \n",
      "     |  renameVariable(...)\n",
      "     |      **`renameVariable(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Variable` named `oldname` to `newname`\n",
      "     |  \n",
      "     |  set_always_mask(...)\n",
      "     |      **`set_always_mask(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_always_mask` for all variables contained in\n",
      "     |      this `netCDF4.Dataset` or `netCDF4.Group`, as well as for all\n",
      "     |      variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion of\n",
      "     |      masked arrays with no missing values to regular ararys shall be\n",
      "     |      applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing\n",
      "     |      variables. Variables created after calling this function will follow\n",
      "     |      the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_chartostring(...)\n",
      "     |      **`set_auto_chartostring(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_chartostring` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion of\n",
      "     |      all character arrays <--> string arrays should be performed for\n",
      "     |      character variables (variables of type `NC_CHAR` or `S1`) with the\n",
      "     |      `_Encoding` attribute set.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_mask(...)\n",
      "     |      **`set_auto_mask(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_mask` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion to masked arrays\n",
      "     |      shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_maskandscale(...)\n",
      "     |      **`set_auto_maskandscale(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_maskandscale` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion to masked arrays\n",
      "     |      and variable scaling shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_scale(...)\n",
      "     |      **`set_auto_scale(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_scale` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic variable scaling\n",
      "     |      shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_fill_off(...)\n",
      "     |      **`set_fill_off(self)`**\n",
      "     |      \n",
      "     |      Sets the fill mode for a `netCDF4.Dataset` open for writing to `off`.\n",
      "     |      \n",
      "     |      This will prevent the data from being pre-filled with fill values, which\n",
      "     |      may result in some performance improvements. However, you must then make\n",
      "     |      sure the data is actually written before being read.\n",
      "     |  \n",
      "     |  set_fill_on(...)\n",
      "     |      **`set_fill_on(self)`**\n",
      "     |      \n",
      "     |      Sets the fill mode for a `netCDF4.Dataset` open for writing to `on`.\n",
      "     |      \n",
      "     |      This causes data to be pre-filled with fill values. The fill values can be\n",
      "     |      controlled by the variable's `_Fill_Value` attribute, but is usually\n",
      "     |      sufficient to the use the netCDF default `_Fill_Value` (defined\n",
      "     |      separately for each variable type). The default behavior of the netCDF\n",
      "     |      library corresponds to `set_fill_on`.  Data which are equal to the\n",
      "     |      `_Fill_Value` indicate that the variable was created, but never written\n",
      "     |      to.\n",
      "     |  \n",
      "     |  set_ncstring_attrs(...)\n",
      "     |      **`set_ncstring_attrs(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_ncstring_attrs` for all variables contained in\n",
      "     |      this `netCDF4.Dataset` or `netCDF4.Group`, as well as for all its\n",
      "     |      subgroups and their variables.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if all string attributes are\n",
      "     |      created as variable-length NC_STRINGs, (if True), or if ascii text\n",
      "     |      attributes are stored as NC_CHARs (if False; default)\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects newly created attributes\n",
      "     |      of existing (sub-) groups and their variables.\n",
      "     |  \n",
      "     |  setncattr(...)\n",
      "     |      **`setncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF dataset or group attribute using name,value pair.\n",
      "     |      Use if you need to set a netCDF attribute with the\n",
      "     |      with the same name as one of the reserved python attributes.\n",
      "     |  \n",
      "     |  setncattr_string(...)\n",
      "     |      **`setncattr_string(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF dataset or group string attribute using name,value pair.\n",
      "     |      Use if you need to ensure that a netCDF attribute is created with type\n",
      "     |      `NC_STRING` if the file format is `NETCDF4`.\n",
      "     |  \n",
      "     |  setncatts(...)\n",
      "     |      **`setncatts(self,attdict)`**\n",
      "     |      \n",
      "     |      set a bunch of netCDF dataset or group attributes at once using a python dictionary.\n",
      "     |      This may be faster when setting a lot of attributes for a `NETCDF3`\n",
      "     |      formatted file, since nc_redef/nc_enddef is not called in between setting\n",
      "     |      each attribute\n",
      "     |  \n",
      "     |  sync(...)\n",
      "     |      **`sync(self)`**\n",
      "     |      \n",
      "     |      Writes all buffered data in the `netCDF4.Dataset` to the disk file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __orthogonal_indexing__\n",
      "     |  \n",
      "     |  cmptypes\n",
      "     |  \n",
      "     |  data_model\n",
      "     |  \n",
      "     |  dimensions\n",
      "     |  \n",
      "     |  disk_format\n",
      "     |  \n",
      "     |  enumtypes\n",
      "     |  \n",
      "     |  file_format\n",
      "     |  \n",
      "     |  groups\n",
      "     |  \n",
      "     |  keepweakref\n",
      "     |  \n",
      "     |  parent\n",
      "     |  \n",
      "     |  path\n",
      "     |  \n",
      "     |  variables\n",
      "     |  \n",
      "     |  vltypes\n",
      "    \n",
      "    class Dimension(builtins.object)\n",
      "     |  A netCDF `netCDF4.Dimension` is used to describe the coordinates of a `netCDF4.Variable`.\n",
      "     |  See `netCDF4.Dimension.__init__` for more details.\n",
      "     |  \n",
      "     |  The current maximum size of a `netCDF4.Dimension` instance can be obtained by\n",
      "     |  calling the python `len` function on the `netCDF4.Dimension` instance. The\n",
      "     |  `netCDF4.Dimension.isunlimited` method of a `netCDF4.Dimension` instance can be used to\n",
      "     |  determine if the dimension is unlimited.\n",
      "     |  \n",
      "     |  Read-only class variables:\n",
      "     |  \n",
      "     |  **`name`**: String name, used when creating a `netCDF4.Variable` with\n",
      "     |  `netCDF4.Dataset.createVariable`.\n",
      "     |  \n",
      "     |  **`size`**: Current `netCDF4.Dimension` size (same as `len(d)`, where `d` is a\n",
      "     |  `netCDF4.Dimension` instance).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      **`__init__(self, group, name, size=None)`**\n",
      "     |      \n",
      "     |      `netCDF4.Dimension` constructor.\n",
      "     |      \n",
      "     |      **`group`**: `netCDF4.Group` instance to associate with dimension.\n",
      "     |      \n",
      "     |      **`name`**: Name of the dimension.\n",
      "     |      \n",
      "     |      **`size`**: Size of the dimension. `None` or 0 means unlimited. (Default `None`).\n",
      "     |      \n",
      "     |      ***Note***: `netCDF4.Dimension` instances should be created using the\n",
      "     |      `netCDF4.Dataset.createDimension` method of a `netCDF4.Group` or\n",
      "     |      `netCDF4.Dataset` instance, not using `netCDF4.Dimension.__init__` directly.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __reduce__ = __reduce_cython__(...)\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  group(...)\n",
      "     |      **`group(self)`**\n",
      "     |      \n",
      "     |      return the group that this `netCDF4.Dimension` is a member of.\n",
      "     |  \n",
      "     |  isunlimited(...)\n",
      "     |      **`isunlimited(self)`**\n",
      "     |      \n",
      "     |      returns `True` if the `netCDF4.Dimension` instance is unlimited, `False` otherwise.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  name\n",
      "     |      string name of Dimension instance\n",
      "     |  \n",
      "     |  size\n",
      "     |      current size of Dimension (calls `len` on Dimension instance)\n",
      "    \n",
      "    class EnumType(builtins.object)\n",
      "     |  A `netCDF4.EnumType` instance is used to describe an Enum data\n",
      "     |  type, and can be passed to the the `netCDF4.Dataset.createVariable` method of\n",
      "     |  a `netCDF4.Dataset` or `netCDF4.Group` instance. See\n",
      "     |  `netCDF4.EnumType.__init__` for more details.\n",
      "     |  \n",
      "     |  The instance variables `dtype`, `name` and `enum_dict` should not be modified by\n",
      "     |  the user.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      **`__init__(group, datatype, datatype_name, enum_dict)`**\n",
      "     |      \n",
      "     |      EnumType constructor.\n",
      "     |      \n",
      "     |      **`group`**: `netCDF4.Group` instance to associate with the VLEN datatype.\n",
      "     |      \n",
      "     |      **`datatype`**: An numpy integer dtype object describing the base type\n",
      "     |      for the Enum.\n",
      "     |      \n",
      "     |      **`datatype_name`**: a Python string containing a description of the\n",
      "     |      Enum data type.\n",
      "     |      \n",
      "     |      **`enum_dict`**: a Python dictionary containing the Enum field/value\n",
      "     |      pairs.\n",
      "     |      \n",
      "     |      ***`Note`***: `netCDF4.EnumType` instances should be created using the\n",
      "     |      `netCDF4.Dataset.createEnumType`\n",
      "     |      method of a `netCDF4.Dataset` or `netCDF4.Group` instance, not using this class directly.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  enum_dict\n",
      "     |  \n",
      "     |  name\n",
      "    \n",
      "    class Group(Dataset)\n",
      "     |  Groups define a hierarchical namespace within a netCDF file. They are\n",
      "     |  analogous to directories in a unix filesystem. Each `netCDF4.Group` behaves like\n",
      "     |  a `netCDF4.Dataset` within a Dataset, and can contain it's own variables,\n",
      "     |  dimensions and attributes (and other Groups). See `netCDF4.Group.__init__`\n",
      "     |  for more details.\n",
      "     |  \n",
      "     |  `netCDF4.Group` inherits from `netCDF4.Dataset`, so all the\n",
      "     |  `netCDF4.Dataset` class methods and variables are available\n",
      "     |  to a `netCDF4.Group` instance (except the `close` method).\n",
      "     |  \n",
      "     |  Additional read-only class variables:\n",
      "     |  \n",
      "     |  **`name`**: String describing the group name.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Group\n",
      "     |      Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      **`__init__(self, parent, name)`**\n",
      "     |      `netCDF4.Group` constructor.\n",
      "     |      \n",
      "     |      **`parent`**: `netCDF4.Group` instance for the parent group.  If being created\n",
      "     |      in the root group, use a `netCDF4.Dataset` instance.\n",
      "     |      \n",
      "     |      **`name`**: - Name of the group.\n",
      "     |      \n",
      "     |      ***Note***: `netCDF4.Group` instances should be created using the\n",
      "     |      `netCDF4.Dataset.createGroup` method of a `netCDF4.Dataset` instance, or\n",
      "     |      another `netCDF4.Group` instance, not using this class directly.\n",
      "     |  \n",
      "     |  close(...)\n",
      "     |      **`close(self)`**\n",
      "     |      \n",
      "     |      overrides `netCDF4.Dataset` close method which does not apply to `netCDF4.Group`\n",
      "     |      instances, raises IOError.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  name\n",
      "     |      string name of Group instance\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __enter__(...)\n",
      "     |  \n",
      "     |  __exit__(...)\n",
      "     |  \n",
      "     |  __getattr__(...)\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  createCompoundType(...)\n",
      "     |      **`createCompoundType(self, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      Creates a new compound data type named `datatype_name` from the numpy\n",
      "     |      dtype object `datatype`.\n",
      "     |      \n",
      "     |      ***Note***: If the new compound data type contains other compound data types\n",
      "     |      (i.e. it is a 'nested' compound type, where not all of the elements\n",
      "     |      are homogeneous numeric data types), then the 'inner' compound types **must** be\n",
      "     |      created first.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.CompoundType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createDimension(...)\n",
      "     |      **`createDimension(self, dimname, size=None)`**\n",
      "     |      \n",
      "     |      Creates a new dimension with the given `dimname` and `size`.\n",
      "     |      \n",
      "     |      `size` must be a positive integer or `None`, which stands for\n",
      "     |      \"unlimited\" (default is `None`). Specifying a size of 0 also\n",
      "     |      results in an unlimited dimension. The return value is the `netCDF4.Dimension`\n",
      "     |      class instance describing the new dimension.  To determine the current\n",
      "     |      maximum size of the dimension, use the `len` function on the `netCDF4.Dimension`\n",
      "     |      instance. To determine if a dimension is 'unlimited', use the\n",
      "     |      `netCDF4.Dimension.isunlimited` method of the `netCDF4.Dimension` instance.\n",
      "     |  \n",
      "     |  createEnumType(...)\n",
      "     |      **`createEnumType(self, datatype, datatype_name, enum_dict)`**\n",
      "     |      \n",
      "     |      Creates a new Enum data type named `datatype_name` from a numpy\n",
      "     |      integer dtype object `datatype`, and a python dictionary\n",
      "     |      defining the enum fields and values.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.EnumType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createGroup(...)\n",
      "     |      **`createGroup(self, groupname)`**\n",
      "     |      \n",
      "     |      Creates a new `netCDF4.Group` with the given `groupname`.\n",
      "     |      \n",
      "     |      If `groupname` is specified as a path, using forward slashes as in unix to\n",
      "     |      separate components, then intermediate groups will be created as necessary\n",
      "     |      (analogous to `mkdir -p` in unix).  For example,\n",
      "     |      `createGroup('/GroupA/GroupB/GroupC')` will create `GroupA`,\n",
      "     |      `GroupA/GroupB`, and `GroupA/GroupB/GroupC`, if they don't already exist.\n",
      "     |      If the specified path describes a group that already exists, no error is\n",
      "     |      raised.\n",
      "     |      \n",
      "     |      The return value is a `netCDF4.Group` class instance.\n",
      "     |  \n",
      "     |  createVLType(...)\n",
      "     |      **`createVLType(self, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      Creates a new VLEN data type named `datatype_name` from a numpy\n",
      "     |      dtype object `datatype`.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.VLType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createVariable(...)\n",
      "     |      **`createVariable(self, varname, datatype, dimensions=(), zlib=False,\n",
      "     |      complevel=4, shuffle=True, fletcher32=False, contiguous=False, chunksizes=None,\n",
      "     |      endian='native', least_significant_digit=None, fill_value=None)`**\n",
      "     |      \n",
      "     |      Creates a new variable with the given `varname`, `datatype`, and\n",
      "     |      `dimensions`. If dimensions are not given, the variable is assumed to be\n",
      "     |      a scalar.\n",
      "     |      \n",
      "     |      If `varname` is specified as a path, using forward slashes as in unix to\n",
      "     |      separate components, then intermediate groups will be created as necessary\n",
      "     |      For example, `createVariable('/GroupA/GroupB/VarC', float, ('x','y'))` will create groups `GroupA`\n",
      "     |      and `GroupA/GroupB`, plus the variable `GroupA/GroupB/VarC`, if the preceding\n",
      "     |      groups don't already exist.\n",
      "     |      \n",
      "     |      The `datatype` can be a numpy datatype object, or a string that describes\n",
      "     |      a numpy dtype object (like the `dtype.str` attribute of a numpy array).\n",
      "     |      Supported specifiers include: `'S1' or 'c' (NC_CHAR), 'i1' or 'b' or 'B'\n",
      "     |      (NC_BYTE), 'u1' (NC_UBYTE), 'i2' or 'h' or 's' (NC_SHORT), 'u2'\n",
      "     |      (NC_USHORT), 'i4' or 'i' or 'l' (NC_INT), 'u4' (NC_UINT), 'i8' (NC_INT64),\n",
      "     |      'u8' (NC_UINT64), 'f4' or 'f' (NC_FLOAT), 'f8' or 'd' (NC_DOUBLE)`.\n",
      "     |      `datatype` can also be a `netCDF4.CompoundType` instance\n",
      "     |      (for a structured, or compound array), a `netCDF4.VLType` instance\n",
      "     |      (for a variable-length array), or the python `str` builtin\n",
      "     |      (for a variable-length string array). Numpy string and unicode datatypes with\n",
      "     |      length greater than one are aliases for `str`.\n",
      "     |      \n",
      "     |      Data from netCDF variables is presented to python as numpy arrays with\n",
      "     |      the corresponding data type.\n",
      "     |      \n",
      "     |      `dimensions` must be a tuple containing dimension names (strings) that\n",
      "     |      have been defined previously using `netCDF4.Dataset.createDimension`. The default value\n",
      "     |      is an empty tuple, which means the variable is a scalar.\n",
      "     |      \n",
      "     |      If the optional keyword `zlib` is `True`, the data will be compressed in\n",
      "     |      the netCDF file using gzip compression (default `False`).\n",
      "     |      \n",
      "     |      The optional keyword `complevel` is an integer between 1 and 9 describing\n",
      "     |      the level of compression desired (default 4). Ignored if `zlib=False`.\n",
      "     |      \n",
      "     |      If the optional keyword `shuffle` is `True`, the HDF5 shuffle filter\n",
      "     |      will be applied before compressing the data (default `True`).  This\n",
      "     |      significantly improves compression. Default is `True`. Ignored if\n",
      "     |      `zlib=False`.\n",
      "     |      \n",
      "     |      If the optional keyword `fletcher32` is `True`, the Fletcher32 HDF5\n",
      "     |      checksum algorithm is activated to detect errors. Default `False`.\n",
      "     |      \n",
      "     |      If the optional keyword `contiguous` is `True`, the variable data is\n",
      "     |      stored contiguously on disk.  Default `False`. Setting to `True` for\n",
      "     |      a variable with an unlimited dimension will trigger an error.\n",
      "     |      \n",
      "     |      The optional keyword `chunksizes` can be used to manually specify the\n",
      "     |      HDF5 chunksizes for each dimension of the variable. A detailed\n",
      "     |      discussion of HDF chunking and I/O performance is available\n",
      "     |      [here](http://www.hdfgroup.org/HDF5/doc/H5.user/Chunking.html).\n",
      "     |      Basically, you want the chunk size for each dimension to match as\n",
      "     |      closely as possible the size of the data block that users will read\n",
      "     |      from the file.  `chunksizes` cannot be set if `contiguous=True`.\n",
      "     |      \n",
      "     |      The optional keyword `endian` can be used to control whether the\n",
      "     |      data is stored in little or big endian format on disk. Possible\n",
      "     |      values are `little, big` or `native` (default). The library\n",
      "     |      will automatically handle endian conversions when the data is read,\n",
      "     |      but if the data is always going to be read on a computer with the\n",
      "     |      opposite format as the one used to create the file, there may be\n",
      "     |      some performance advantage to be gained by setting the endian-ness.\n",
      "     |      \n",
      "     |      The `zlib, complevel, shuffle, fletcher32, contiguous, chunksizes` and `endian`\n",
      "     |      keywords are silently ignored for netCDF 3 files that do not use HDF5.\n",
      "     |      \n",
      "     |      The optional keyword `fill_value` can be used to override the default\n",
      "     |      netCDF `_FillValue` (the value that the variable gets filled with before\n",
      "     |      any data is written to it, defaults given in `netCDF4.default_fillvals`).\n",
      "     |      If fill_value is set to `False`, then the variable is not pre-filled.\n",
      "     |      \n",
      "     |      If the optional keyword parameter `least_significant_digit` is\n",
      "     |      specified, variable data will be truncated (quantized). In conjunction\n",
      "     |      with `zlib=True` this produces 'lossy', but significantly more\n",
      "     |      efficient compression. For example, if `least_significant_digit=1`,\n",
      "     |      data will be quantized using `numpy.around(scale*data)/scale`, where\n",
      "     |      scale = 2**bits, and bits is determined so that a precision of 0.1 is\n",
      "     |      retained (in this case bits=4). From the\n",
      "     |      [PSD metadata conventions](http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml):\n",
      "     |      \"least_significant_digit -- power of ten of the smallest decimal place\n",
      "     |      in unpacked data that is a reliable value.\" Default is `None`, or no\n",
      "     |      quantization, or 'lossless' compression.\n",
      "     |      \n",
      "     |      When creating variables in a `NETCDF4` or `NETCDF4_CLASSIC` formatted file,\n",
      "     |      HDF5 creates something called a 'chunk cache' for each variable.  The\n",
      "     |      default size of the chunk cache may be large enough to completely fill\n",
      "     |      available memory when creating thousands of variables.  The optional\n",
      "     |      keyword `chunk_cache` allows you to reduce (or increase) the size of\n",
      "     |      the default chunk cache when creating a variable.  The setting only\n",
      "     |      persists as long as the Dataset is open - you can use the set_var_chunk_cache\n",
      "     |      method to change it the next time the Dataset is opened.\n",
      "     |      Warning - messing with this parameter can seriously degrade performance.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.Variable` class instance describing the new\n",
      "     |      variable.\n",
      "     |      \n",
      "     |      A list of names corresponding to netCDF variable attributes can be\n",
      "     |      obtained with the `netCDF4.Variable` method `netCDF4.Variable.ncattrs`. A dictionary\n",
      "     |      containing all the netCDF attribute name/value pairs is provided by\n",
      "     |      the `__dict__` attribute of a `netCDF4.Variable` instance.\n",
      "     |      \n",
      "     |      `netCDF4.Variable` instances behave much like array objects. Data can be\n",
      "     |      assigned to or retrieved from a variable with indexing and slicing\n",
      "     |      operations on the `netCDF4.Variable` instance. A `netCDF4.Variable` instance has six\n",
      "     |      Dataset standard attributes: `dimensions, dtype, shape, ndim, name` and\n",
      "     |      `least_significant_digit`. Application programs should never modify\n",
      "     |      these attributes. The `dimensions` attribute is a tuple containing the\n",
      "     |      names of the dimensions associated with this variable. The `dtype`\n",
      "     |      attribute is a string describing the variable's data type (`i4, f8,\n",
      "     |      S1,` etc). The `shape` attribute is a tuple describing the current\n",
      "     |      sizes of all the variable's dimensions. The `name` attribute is a\n",
      "     |      string containing the name of the Variable instance.\n",
      "     |      The `least_significant_digit`\n",
      "     |      attributes describes the power of ten of the smallest decimal place in\n",
      "     |      the data the contains a reliable value.  assigned to the `netCDF4.Variable`\n",
      "     |      instance. If `None`, the data is not truncated. The `ndim` attribute\n",
      "     |      is the number of variable dimensions.\n",
      "     |  \n",
      "     |  delncattr(...)\n",
      "     |      **`delncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      delete a netCDF dataset or group attribute.  Use if you need to delete a\n",
      "     |      netCDF attribute with the same name as one of the reserved python\n",
      "     |      attributes.\n",
      "     |  \n",
      "     |  filepath(...)\n",
      "     |      **`filepath(self,encoding=None)`**\n",
      "     |      \n",
      "     |      Get the file system path (or the opendap URL) which was used to\n",
      "     |      open/create the Dataset. Requires netcdf >= 4.1.2.  The path\n",
      "     |      is decoded into a string using `sys.getfilesystemencoding()` by default, this can be\n",
      "     |      changed using the `encoding` kwarg.\n",
      "     |  \n",
      "     |  get_variables_by_attributes(...)\n",
      "     |      **`get_variables_by_attribute(self, **kwargs)`**\n",
      "     |      \n",
      "     |      Returns a list of variables that match specific conditions.\n",
      "     |      \n",
      "     |      Can pass in key=value parameters and variables are returned that\n",
      "     |      contain all of the matches. For example,\n",
      "     |      \n",
      "     |          :::python\n",
      "     |          >>> # Get variables with x-axis attribute.\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis='X')\n",
      "     |          >>> # Get variables with matching \"standard_name\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n",
      "     |      \n",
      "     |      Can pass in key=callable parameter and variables are returned if the\n",
      "     |      callable returns True.  The callable should accept a single parameter,\n",
      "     |      the attribute value.  None is given as the attribute value when the\n",
      "     |      attribute does not exist on the variable. For example,\n",
      "     |      \n",
      "     |          :::python\n",
      "     |          >>> # Get Axis variables\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis=lambda v: v in ['X', 'Y', 'Z', 'T'])\n",
      "     |          >>> # Get variables that don't have an \"axis\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis=lambda v: v is None)\n",
      "     |          >>> # Get variables that have a \"grid_mapping\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(grid_mapping=lambda v: v is not None)\n",
      "     |  \n",
      "     |  getncattr(...)\n",
      "     |      **`getncattr(self,name)`**\n",
      "     |      \n",
      "     |      retrieve a netCDF dataset or group attribute.\n",
      "     |      Use if you need to get a netCDF attribute with the same\n",
      "     |      name as one of the reserved python attributes.\n",
      "     |      \n",
      "     |      option kwarg `encoding` can be used to specify the\n",
      "     |      character encoding of a string attribute (default is `utf-8`).\n",
      "     |  \n",
      "     |  isopen(...)\n",
      "     |      **`close(self)`**\n",
      "     |      \n",
      "     |      is the Dataset open or closed?\n",
      "     |  \n",
      "     |  ncattrs(...)\n",
      "     |      **`ncattrs(self)`**\n",
      "     |      \n",
      "     |      return netCDF global attribute names for this `netCDF4.Dataset` or `netCDF4.Group` in a list.\n",
      "     |  \n",
      "     |  renameAttribute(...)\n",
      "     |      **`renameAttribute(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Dataset` or `netCDF4.Group` attribute named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  renameDimension(...)\n",
      "     |      **`renameDimension(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Dimension` named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  renameGroup(...)\n",
      "     |      **`renameGroup(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Group` named `oldname` to `newname` (requires netcdf >= 4.3.1).\n",
      "     |  \n",
      "     |  renameVariable(...)\n",
      "     |      **`renameVariable(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Variable` named `oldname` to `newname`\n",
      "     |  \n",
      "     |  set_always_mask(...)\n",
      "     |      **`set_always_mask(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_always_mask` for all variables contained in\n",
      "     |      this `netCDF4.Dataset` or `netCDF4.Group`, as well as for all\n",
      "     |      variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion of\n",
      "     |      masked arrays with no missing values to regular ararys shall be\n",
      "     |      applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing\n",
      "     |      variables. Variables created after calling this function will follow\n",
      "     |      the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_chartostring(...)\n",
      "     |      **`set_auto_chartostring(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_chartostring` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion of\n",
      "     |      all character arrays <--> string arrays should be performed for\n",
      "     |      character variables (variables of type `NC_CHAR` or `S1`) with the\n",
      "     |      `_Encoding` attribute set.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_mask(...)\n",
      "     |      **`set_auto_mask(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_mask` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion to masked arrays\n",
      "     |      shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_maskandscale(...)\n",
      "     |      **`set_auto_maskandscale(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_maskandscale` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion to masked arrays\n",
      "     |      and variable scaling shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_scale(...)\n",
      "     |      **`set_auto_scale(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_scale` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic variable scaling\n",
      "     |      shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_fill_off(...)\n",
      "     |      **`set_fill_off(self)`**\n",
      "     |      \n",
      "     |      Sets the fill mode for a `netCDF4.Dataset` open for writing to `off`.\n",
      "     |      \n",
      "     |      This will prevent the data from being pre-filled with fill values, which\n",
      "     |      may result in some performance improvements. However, you must then make\n",
      "     |      sure the data is actually written before being read.\n",
      "     |  \n",
      "     |  set_fill_on(...)\n",
      "     |      **`set_fill_on(self)`**\n",
      "     |      \n",
      "     |      Sets the fill mode for a `netCDF4.Dataset` open for writing to `on`.\n",
      "     |      \n",
      "     |      This causes data to be pre-filled with fill values. The fill values can be\n",
      "     |      controlled by the variable's `_Fill_Value` attribute, but is usually\n",
      "     |      sufficient to the use the netCDF default `_Fill_Value` (defined\n",
      "     |      separately for each variable type). The default behavior of the netCDF\n",
      "     |      library corresponds to `set_fill_on`.  Data which are equal to the\n",
      "     |      `_Fill_Value` indicate that the variable was created, but never written\n",
      "     |      to.\n",
      "     |  \n",
      "     |  set_ncstring_attrs(...)\n",
      "     |      **`set_ncstring_attrs(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_ncstring_attrs` for all variables contained in\n",
      "     |      this `netCDF4.Dataset` or `netCDF4.Group`, as well as for all its\n",
      "     |      subgroups and their variables.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if all string attributes are\n",
      "     |      created as variable-length NC_STRINGs, (if True), or if ascii text\n",
      "     |      attributes are stored as NC_CHARs (if False; default)\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects newly created attributes\n",
      "     |      of existing (sub-) groups and their variables.\n",
      "     |  \n",
      "     |  setncattr(...)\n",
      "     |      **`setncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF dataset or group attribute using name,value pair.\n",
      "     |      Use if you need to set a netCDF attribute with the\n",
      "     |      with the same name as one of the reserved python attributes.\n",
      "     |  \n",
      "     |  setncattr_string(...)\n",
      "     |      **`setncattr_string(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF dataset or group string attribute using name,value pair.\n",
      "     |      Use if you need to ensure that a netCDF attribute is created with type\n",
      "     |      `NC_STRING` if the file format is `NETCDF4`.\n",
      "     |  \n",
      "     |  setncatts(...)\n",
      "     |      **`setncatts(self,attdict)`**\n",
      "     |      \n",
      "     |      set a bunch of netCDF dataset or group attributes at once using a python dictionary.\n",
      "     |      This may be faster when setting a lot of attributes for a `NETCDF3`\n",
      "     |      formatted file, since nc_redef/nc_enddef is not called in between setting\n",
      "     |      each attribute\n",
      "     |  \n",
      "     |  sync(...)\n",
      "     |      **`sync(self)`**\n",
      "     |      \n",
      "     |      Writes all buffered data in the `netCDF4.Dataset` to the disk file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __orthogonal_indexing__\n",
      "     |  \n",
      "     |  cmptypes\n",
      "     |  \n",
      "     |  data_model\n",
      "     |  \n",
      "     |  dimensions\n",
      "     |  \n",
      "     |  disk_format\n",
      "     |  \n",
      "     |  enumtypes\n",
      "     |  \n",
      "     |  file_format\n",
      "     |  \n",
      "     |  groups\n",
      "     |  \n",
      "     |  keepweakref\n",
      "     |  \n",
      "     |  parent\n",
      "     |  \n",
      "     |  path\n",
      "     |  \n",
      "     |  variables\n",
      "     |  \n",
      "     |  vltypes\n",
      "    \n",
      "    class MFDataset(Dataset)\n",
      "     |  MFDataset(files, check=False, aggdim=None, exclude=[], master_file=None)\n",
      "     |  \n",
      "     |  Class for reading multi-file netCDF Datasets, making variables\n",
      "     |  spanning multiple files appear as if they were in one file.\n",
      "     |  Datasets must be in `NETCDF4_CLASSIC, NETCDF3_CLASSIC, NETCDF3_64BIT_OFFSET\n",
      "     |  or NETCDF3_64BIT_DATA` format (`NETCDF4` Datasets won't work).\n",
      "     |  \n",
      "     |  Adapted from [pycdf](http://pysclint.sourceforge.net/pycdf) by Andre Gosselin.\n",
      "     |  \n",
      "     |  Example usage (See `netCDF4.MFDataset.__init__` for more details):\n",
      "     |  \n",
      "     |      :::python\n",
      "     |      >>> import numpy as np\n",
      "     |      >>> # create a series of netCDF files with a variable sharing\n",
      "     |      >>> # the same unlimited dimension.\n",
      "     |      >>> for nf in range(10):\n",
      "     |      >>>     f = Dataset(\"mftest%s.nc\" % nf,\"w\",format='NETCDF4_CLASSIC')\n",
      "     |      >>>     f.createDimension(\"x\",None)\n",
      "     |      >>>     x = f.createVariable(\"x\",\"i\",(\"x\",))\n",
      "     |      >>>     x[0:10] = np.arange(nf*10,10*(nf+1))\n",
      "     |      >>>     f.close()\n",
      "     |      >>> # now read all those files in at once, in one Dataset.\n",
      "     |      >>> f = MFDataset(\"mftest*nc\")\n",
      "     |      >>> print f.variables[\"x\"][:]\n",
      "     |      [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      "     |       25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      "     |       50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      "     |       75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MFDataset\n",
      "     |      Dataset\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getattribute__(self, name)\n",
      "     |  \n",
      "     |  __init__(self, files, check=False, aggdim=None, exclude=[], master_file=None)\n",
      "     |      **`__init__(self, files, check=False, aggdim=None, exclude=[],\n",
      "     |      master_file=None)`**\n",
      "     |      \n",
      "     |      Open a Dataset spanning multiple files, making it look as if it was a\n",
      "     |      single file. Variables in the list of files that share the same\n",
      "     |      dimension (specified with the keyword `aggdim`) are aggregated. If\n",
      "     |      `aggdim` is not specified, the unlimited is aggregated.  Currently,\n",
      "     |      `aggdim` must be the leftmost (slowest varying) dimension of each\n",
      "     |      of the variables to be aggregated.\n",
      "     |      \n",
      "     |      **`files`**: either a sequence of netCDF files or a string with a\n",
      "     |      wildcard (converted to a sorted list of files using glob)  If\n",
      "     |      the `master_file` kwarg is not specified, the first file\n",
      "     |      in the list will become the \"master\" file, defining all the\n",
      "     |      variables with an aggregation dimension which may span\n",
      "     |      subsequent files. Attribute access returns attributes only from \"master\"\n",
      "     |      file. The files are always opened in read-only mode.\n",
      "     |      \n",
      "     |      **`check`**: True if you want to do consistency checking to ensure the\n",
      "     |      correct variables structure for all of the netcdf files.  Checking makes\n",
      "     |      the initialization of the MFDataset instance much slower. Default is\n",
      "     |      False.\n",
      "     |      \n",
      "     |      **`aggdim`**: The name of the dimension to aggregate over (must\n",
      "     |      be the leftmost dimension of each of the variables to be aggregated).\n",
      "     |      If None (default), aggregate over the unlimited dimension.\n",
      "     |      \n",
      "     |      **`exclude`**: A list of variable names to exclude from aggregation.\n",
      "     |      Default is an empty list.\n",
      "     |      \n",
      "     |      **`master_file`**: file to use as \"master file\", defining all the\n",
      "     |      variables with an aggregation dimension and all global attributes.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      override base class attribute creation\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      **`close(self)`**\n",
      "     |      \n",
      "     |      close all the open files.\n",
      "     |  \n",
      "     |  ncattrs(self)\n",
      "     |      **`ncattrs(self)`**\n",
      "     |      \n",
      "     |      return the netcdf attribute names from the master file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __enter__(...)\n",
      "     |  \n",
      "     |  __exit__(...)\n",
      "     |  \n",
      "     |  __getattr__(...)\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  createCompoundType(...)\n",
      "     |      **`createCompoundType(self, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      Creates a new compound data type named `datatype_name` from the numpy\n",
      "     |      dtype object `datatype`.\n",
      "     |      \n",
      "     |      ***Note***: If the new compound data type contains other compound data types\n",
      "     |      (i.e. it is a 'nested' compound type, where not all of the elements\n",
      "     |      are homogeneous numeric data types), then the 'inner' compound types **must** be\n",
      "     |      created first.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.CompoundType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createDimension(...)\n",
      "     |      **`createDimension(self, dimname, size=None)`**\n",
      "     |      \n",
      "     |      Creates a new dimension with the given `dimname` and `size`.\n",
      "     |      \n",
      "     |      `size` must be a positive integer or `None`, which stands for\n",
      "     |      \"unlimited\" (default is `None`). Specifying a size of 0 also\n",
      "     |      results in an unlimited dimension. The return value is the `netCDF4.Dimension`\n",
      "     |      class instance describing the new dimension.  To determine the current\n",
      "     |      maximum size of the dimension, use the `len` function on the `netCDF4.Dimension`\n",
      "     |      instance. To determine if a dimension is 'unlimited', use the\n",
      "     |      `netCDF4.Dimension.isunlimited` method of the `netCDF4.Dimension` instance.\n",
      "     |  \n",
      "     |  createEnumType(...)\n",
      "     |      **`createEnumType(self, datatype, datatype_name, enum_dict)`**\n",
      "     |      \n",
      "     |      Creates a new Enum data type named `datatype_name` from a numpy\n",
      "     |      integer dtype object `datatype`, and a python dictionary\n",
      "     |      defining the enum fields and values.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.EnumType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createGroup(...)\n",
      "     |      **`createGroup(self, groupname)`**\n",
      "     |      \n",
      "     |      Creates a new `netCDF4.Group` with the given `groupname`.\n",
      "     |      \n",
      "     |      If `groupname` is specified as a path, using forward slashes as in unix to\n",
      "     |      separate components, then intermediate groups will be created as necessary\n",
      "     |      (analogous to `mkdir -p` in unix).  For example,\n",
      "     |      `createGroup('/GroupA/GroupB/GroupC')` will create `GroupA`,\n",
      "     |      `GroupA/GroupB`, and `GroupA/GroupB/GroupC`, if they don't already exist.\n",
      "     |      If the specified path describes a group that already exists, no error is\n",
      "     |      raised.\n",
      "     |      \n",
      "     |      The return value is a `netCDF4.Group` class instance.\n",
      "     |  \n",
      "     |  createVLType(...)\n",
      "     |      **`createVLType(self, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      Creates a new VLEN data type named `datatype_name` from a numpy\n",
      "     |      dtype object `datatype`.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.VLType` class instance describing the new\n",
      "     |      datatype.\n",
      "     |  \n",
      "     |  createVariable(...)\n",
      "     |      **`createVariable(self, varname, datatype, dimensions=(), zlib=False,\n",
      "     |      complevel=4, shuffle=True, fletcher32=False, contiguous=False, chunksizes=None,\n",
      "     |      endian='native', least_significant_digit=None, fill_value=None)`**\n",
      "     |      \n",
      "     |      Creates a new variable with the given `varname`, `datatype`, and\n",
      "     |      `dimensions`. If dimensions are not given, the variable is assumed to be\n",
      "     |      a scalar.\n",
      "     |      \n",
      "     |      If `varname` is specified as a path, using forward slashes as in unix to\n",
      "     |      separate components, then intermediate groups will be created as necessary\n",
      "     |      For example, `createVariable('/GroupA/GroupB/VarC', float, ('x','y'))` will create groups `GroupA`\n",
      "     |      and `GroupA/GroupB`, plus the variable `GroupA/GroupB/VarC`, if the preceding\n",
      "     |      groups don't already exist.\n",
      "     |      \n",
      "     |      The `datatype` can be a numpy datatype object, or a string that describes\n",
      "     |      a numpy dtype object (like the `dtype.str` attribute of a numpy array).\n",
      "     |      Supported specifiers include: `'S1' or 'c' (NC_CHAR), 'i1' or 'b' or 'B'\n",
      "     |      (NC_BYTE), 'u1' (NC_UBYTE), 'i2' or 'h' or 's' (NC_SHORT), 'u2'\n",
      "     |      (NC_USHORT), 'i4' or 'i' or 'l' (NC_INT), 'u4' (NC_UINT), 'i8' (NC_INT64),\n",
      "     |      'u8' (NC_UINT64), 'f4' or 'f' (NC_FLOAT), 'f8' or 'd' (NC_DOUBLE)`.\n",
      "     |      `datatype` can also be a `netCDF4.CompoundType` instance\n",
      "     |      (for a structured, or compound array), a `netCDF4.VLType` instance\n",
      "     |      (for a variable-length array), or the python `str` builtin\n",
      "     |      (for a variable-length string array). Numpy string and unicode datatypes with\n",
      "     |      length greater than one are aliases for `str`.\n",
      "     |      \n",
      "     |      Data from netCDF variables is presented to python as numpy arrays with\n",
      "     |      the corresponding data type.\n",
      "     |      \n",
      "     |      `dimensions` must be a tuple containing dimension names (strings) that\n",
      "     |      have been defined previously using `netCDF4.Dataset.createDimension`. The default value\n",
      "     |      is an empty tuple, which means the variable is a scalar.\n",
      "     |      \n",
      "     |      If the optional keyword `zlib` is `True`, the data will be compressed in\n",
      "     |      the netCDF file using gzip compression (default `False`).\n",
      "     |      \n",
      "     |      The optional keyword `complevel` is an integer between 1 and 9 describing\n",
      "     |      the level of compression desired (default 4). Ignored if `zlib=False`.\n",
      "     |      \n",
      "     |      If the optional keyword `shuffle` is `True`, the HDF5 shuffle filter\n",
      "     |      will be applied before compressing the data (default `True`).  This\n",
      "     |      significantly improves compression. Default is `True`. Ignored if\n",
      "     |      `zlib=False`.\n",
      "     |      \n",
      "     |      If the optional keyword `fletcher32` is `True`, the Fletcher32 HDF5\n",
      "     |      checksum algorithm is activated to detect errors. Default `False`.\n",
      "     |      \n",
      "     |      If the optional keyword `contiguous` is `True`, the variable data is\n",
      "     |      stored contiguously on disk.  Default `False`. Setting to `True` for\n",
      "     |      a variable with an unlimited dimension will trigger an error.\n",
      "     |      \n",
      "     |      The optional keyword `chunksizes` can be used to manually specify the\n",
      "     |      HDF5 chunksizes for each dimension of the variable. A detailed\n",
      "     |      discussion of HDF chunking and I/O performance is available\n",
      "     |      [here](http://www.hdfgroup.org/HDF5/doc/H5.user/Chunking.html).\n",
      "     |      Basically, you want the chunk size for each dimension to match as\n",
      "     |      closely as possible the size of the data block that users will read\n",
      "     |      from the file.  `chunksizes` cannot be set if `contiguous=True`.\n",
      "     |      \n",
      "     |      The optional keyword `endian` can be used to control whether the\n",
      "     |      data is stored in little or big endian format on disk. Possible\n",
      "     |      values are `little, big` or `native` (default). The library\n",
      "     |      will automatically handle endian conversions when the data is read,\n",
      "     |      but if the data is always going to be read on a computer with the\n",
      "     |      opposite format as the one used to create the file, there may be\n",
      "     |      some performance advantage to be gained by setting the endian-ness.\n",
      "     |      \n",
      "     |      The `zlib, complevel, shuffle, fletcher32, contiguous, chunksizes` and `endian`\n",
      "     |      keywords are silently ignored for netCDF 3 files that do not use HDF5.\n",
      "     |      \n",
      "     |      The optional keyword `fill_value` can be used to override the default\n",
      "     |      netCDF `_FillValue` (the value that the variable gets filled with before\n",
      "     |      any data is written to it, defaults given in `netCDF4.default_fillvals`).\n",
      "     |      If fill_value is set to `False`, then the variable is not pre-filled.\n",
      "     |      \n",
      "     |      If the optional keyword parameter `least_significant_digit` is\n",
      "     |      specified, variable data will be truncated (quantized). In conjunction\n",
      "     |      with `zlib=True` this produces 'lossy', but significantly more\n",
      "     |      efficient compression. For example, if `least_significant_digit=1`,\n",
      "     |      data will be quantized using `numpy.around(scale*data)/scale`, where\n",
      "     |      scale = 2**bits, and bits is determined so that a precision of 0.1 is\n",
      "     |      retained (in this case bits=4). From the\n",
      "     |      [PSD metadata conventions](http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml):\n",
      "     |      \"least_significant_digit -- power of ten of the smallest decimal place\n",
      "     |      in unpacked data that is a reliable value.\" Default is `None`, or no\n",
      "     |      quantization, or 'lossless' compression.\n",
      "     |      \n",
      "     |      When creating variables in a `NETCDF4` or `NETCDF4_CLASSIC` formatted file,\n",
      "     |      HDF5 creates something called a 'chunk cache' for each variable.  The\n",
      "     |      default size of the chunk cache may be large enough to completely fill\n",
      "     |      available memory when creating thousands of variables.  The optional\n",
      "     |      keyword `chunk_cache` allows you to reduce (or increase) the size of\n",
      "     |      the default chunk cache when creating a variable.  The setting only\n",
      "     |      persists as long as the Dataset is open - you can use the set_var_chunk_cache\n",
      "     |      method to change it the next time the Dataset is opened.\n",
      "     |      Warning - messing with this parameter can seriously degrade performance.\n",
      "     |      \n",
      "     |      The return value is the `netCDF4.Variable` class instance describing the new\n",
      "     |      variable.\n",
      "     |      \n",
      "     |      A list of names corresponding to netCDF variable attributes can be\n",
      "     |      obtained with the `netCDF4.Variable` method `netCDF4.Variable.ncattrs`. A dictionary\n",
      "     |      containing all the netCDF attribute name/value pairs is provided by\n",
      "     |      the `__dict__` attribute of a `netCDF4.Variable` instance.\n",
      "     |      \n",
      "     |      `netCDF4.Variable` instances behave much like array objects. Data can be\n",
      "     |      assigned to or retrieved from a variable with indexing and slicing\n",
      "     |      operations on the `netCDF4.Variable` instance. A `netCDF4.Variable` instance has six\n",
      "     |      Dataset standard attributes: `dimensions, dtype, shape, ndim, name` and\n",
      "     |      `least_significant_digit`. Application programs should never modify\n",
      "     |      these attributes. The `dimensions` attribute is a tuple containing the\n",
      "     |      names of the dimensions associated with this variable. The `dtype`\n",
      "     |      attribute is a string describing the variable's data type (`i4, f8,\n",
      "     |      S1,` etc). The `shape` attribute is a tuple describing the current\n",
      "     |      sizes of all the variable's dimensions. The `name` attribute is a\n",
      "     |      string containing the name of the Variable instance.\n",
      "     |      The `least_significant_digit`\n",
      "     |      attributes describes the power of ten of the smallest decimal place in\n",
      "     |      the data the contains a reliable value.  assigned to the `netCDF4.Variable`\n",
      "     |      instance. If `None`, the data is not truncated. The `ndim` attribute\n",
      "     |      is the number of variable dimensions.\n",
      "     |  \n",
      "     |  delncattr(...)\n",
      "     |      **`delncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      delete a netCDF dataset or group attribute.  Use if you need to delete a\n",
      "     |      netCDF attribute with the same name as one of the reserved python\n",
      "     |      attributes.\n",
      "     |  \n",
      "     |  filepath(...)\n",
      "     |      **`filepath(self,encoding=None)`**\n",
      "     |      \n",
      "     |      Get the file system path (or the opendap URL) which was used to\n",
      "     |      open/create the Dataset. Requires netcdf >= 4.1.2.  The path\n",
      "     |      is decoded into a string using `sys.getfilesystemencoding()` by default, this can be\n",
      "     |      changed using the `encoding` kwarg.\n",
      "     |  \n",
      "     |  get_variables_by_attributes(...)\n",
      "     |      **`get_variables_by_attribute(self, **kwargs)`**\n",
      "     |      \n",
      "     |      Returns a list of variables that match specific conditions.\n",
      "     |      \n",
      "     |      Can pass in key=value parameters and variables are returned that\n",
      "     |      contain all of the matches. For example,\n",
      "     |      \n",
      "     |          :::python\n",
      "     |          >>> # Get variables with x-axis attribute.\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis='X')\n",
      "     |          >>> # Get variables with matching \"standard_name\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(standard_name='northward_sea_water_velocity')\n",
      "     |      \n",
      "     |      Can pass in key=callable parameter and variables are returned if the\n",
      "     |      callable returns True.  The callable should accept a single parameter,\n",
      "     |      the attribute value.  None is given as the attribute value when the\n",
      "     |      attribute does not exist on the variable. For example,\n",
      "     |      \n",
      "     |          :::python\n",
      "     |          >>> # Get Axis variables\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis=lambda v: v in ['X', 'Y', 'Z', 'T'])\n",
      "     |          >>> # Get variables that don't have an \"axis\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(axis=lambda v: v is None)\n",
      "     |          >>> # Get variables that have a \"grid_mapping\" attribute\n",
      "     |          >>> vs = nc.get_variables_by_attributes(grid_mapping=lambda v: v is not None)\n",
      "     |  \n",
      "     |  getncattr(...)\n",
      "     |      **`getncattr(self,name)`**\n",
      "     |      \n",
      "     |      retrieve a netCDF dataset or group attribute.\n",
      "     |      Use if you need to get a netCDF attribute with the same\n",
      "     |      name as one of the reserved python attributes.\n",
      "     |      \n",
      "     |      option kwarg `encoding` can be used to specify the\n",
      "     |      character encoding of a string attribute (default is `utf-8`).\n",
      "     |  \n",
      "     |  isopen(...)\n",
      "     |      **`close(self)`**\n",
      "     |      \n",
      "     |      is the Dataset open or closed?\n",
      "     |  \n",
      "     |  renameAttribute(...)\n",
      "     |      **`renameAttribute(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Dataset` or `netCDF4.Group` attribute named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  renameDimension(...)\n",
      "     |      **`renameDimension(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Dimension` named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  renameGroup(...)\n",
      "     |      **`renameGroup(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Group` named `oldname` to `newname` (requires netcdf >= 4.3.1).\n",
      "     |  \n",
      "     |  renameVariable(...)\n",
      "     |      **`renameVariable(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Variable` named `oldname` to `newname`\n",
      "     |  \n",
      "     |  set_always_mask(...)\n",
      "     |      **`set_always_mask(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_always_mask` for all variables contained in\n",
      "     |      this `netCDF4.Dataset` or `netCDF4.Group`, as well as for all\n",
      "     |      variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion of\n",
      "     |      masked arrays with no missing values to regular ararys shall be\n",
      "     |      applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing\n",
      "     |      variables. Variables created after calling this function will follow\n",
      "     |      the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_chartostring(...)\n",
      "     |      **`set_auto_chartostring(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_chartostring` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion of\n",
      "     |      all character arrays <--> string arrays should be performed for\n",
      "     |      character variables (variables of type `NC_CHAR` or `S1`) with the\n",
      "     |      `_Encoding` attribute set.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_mask(...)\n",
      "     |      **`set_auto_mask(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_mask` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion to masked arrays\n",
      "     |      shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_maskandscale(...)\n",
      "     |      **`set_auto_maskandscale(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_maskandscale` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic conversion to masked arrays\n",
      "     |      and variable scaling shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_auto_scale(...)\n",
      "     |      **`set_auto_scale(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_auto_scale` for all variables contained in this `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group`, as well as for all variables in all its subgroups.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if automatic variable scaling\n",
      "     |      shall be applied for all variables.\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects existing variables. Variables created\n",
      "     |      after calling this function will follow the default behaviour.\n",
      "     |  \n",
      "     |  set_fill_off(...)\n",
      "     |      **`set_fill_off(self)`**\n",
      "     |      \n",
      "     |      Sets the fill mode for a `netCDF4.Dataset` open for writing to `off`.\n",
      "     |      \n",
      "     |      This will prevent the data from being pre-filled with fill values, which\n",
      "     |      may result in some performance improvements. However, you must then make\n",
      "     |      sure the data is actually written before being read.\n",
      "     |  \n",
      "     |  set_fill_on(...)\n",
      "     |      **`set_fill_on(self)`**\n",
      "     |      \n",
      "     |      Sets the fill mode for a `netCDF4.Dataset` open for writing to `on`.\n",
      "     |      \n",
      "     |      This causes data to be pre-filled with fill values. The fill values can be\n",
      "     |      controlled by the variable's `_Fill_Value` attribute, but is usually\n",
      "     |      sufficient to the use the netCDF default `_Fill_Value` (defined\n",
      "     |      separately for each variable type). The default behavior of the netCDF\n",
      "     |      library corresponds to `set_fill_on`.  Data which are equal to the\n",
      "     |      `_Fill_Value` indicate that the variable was created, but never written\n",
      "     |      to.\n",
      "     |  \n",
      "     |  set_ncstring_attrs(...)\n",
      "     |      **`set_ncstring_attrs(self, True_or_False)`**\n",
      "     |      \n",
      "     |      Call `netCDF4.Variable.set_ncstring_attrs` for all variables contained in\n",
      "     |      this `netCDF4.Dataset` or `netCDF4.Group`, as well as for all its\n",
      "     |      subgroups and their variables.\n",
      "     |      \n",
      "     |      **`True_or_False`**: Boolean determining if all string attributes are\n",
      "     |      created as variable-length NC_STRINGs, (if True), or if ascii text\n",
      "     |      attributes are stored as NC_CHARs (if False; default)\n",
      "     |      \n",
      "     |      ***Note***: Calling this function only affects newly created attributes\n",
      "     |      of existing (sub-) groups and their variables.\n",
      "     |  \n",
      "     |  setncattr(...)\n",
      "     |      **`setncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF dataset or group attribute using name,value pair.\n",
      "     |      Use if you need to set a netCDF attribute with the\n",
      "     |      with the same name as one of the reserved python attributes.\n",
      "     |  \n",
      "     |  setncattr_string(...)\n",
      "     |      **`setncattr_string(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF dataset or group string attribute using name,value pair.\n",
      "     |      Use if you need to ensure that a netCDF attribute is created with type\n",
      "     |      `NC_STRING` if the file format is `NETCDF4`.\n",
      "     |  \n",
      "     |  setncatts(...)\n",
      "     |      **`setncatts(self,attdict)`**\n",
      "     |      \n",
      "     |      set a bunch of netCDF dataset or group attributes at once using a python dictionary.\n",
      "     |      This may be faster when setting a lot of attributes for a `NETCDF3`\n",
      "     |      formatted file, since nc_redef/nc_enddef is not called in between setting\n",
      "     |      each attribute\n",
      "     |  \n",
      "     |  sync(...)\n",
      "     |      **`sync(self)`**\n",
      "     |      \n",
      "     |      Writes all buffered data in the `netCDF4.Dataset` to the disk file.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __orthogonal_indexing__\n",
      "     |  \n",
      "     |  cmptypes\n",
      "     |  \n",
      "     |  data_model\n",
      "     |  \n",
      "     |  dimensions\n",
      "     |  \n",
      "     |  disk_format\n",
      "     |  \n",
      "     |  enumtypes\n",
      "     |  \n",
      "     |  file_format\n",
      "     |  \n",
      "     |  groups\n",
      "     |  \n",
      "     |  keepweakref\n",
      "     |  \n",
      "     |  parent\n",
      "     |  \n",
      "     |  path\n",
      "     |  \n",
      "     |  variables\n",
      "     |  \n",
      "     |  vltypes\n",
      "    \n",
      "    class MFTime(_Variable)\n",
      "     |  MFTime(time, units=None, calendar=None)\n",
      "     |  \n",
      "     |  Class providing an interface to a MFDataset time Variable by imposing a unique common\n",
      "     |  time unit and/or calendar to all files.\n",
      "     |  \n",
      "     |  Example usage (See `netCDF4.MFTime.__init__` for more details):\n",
      "     |  \n",
      "     |      :::python\n",
      "     |      >>> import numpy\n",
      "     |      >>> f1 = Dataset(\"mftest_1.nc\",\"w\", format=\"NETCDF4_CLASSIC\")\n",
      "     |      >>> f2 = Dataset(\"mftest_2.nc\",\"w\", format=\"NETCDF4_CLASSIC\")\n",
      "     |      >>> f1.createDimension(\"time\",None)\n",
      "     |      >>> f2.createDimension(\"time\",None)\n",
      "     |      >>> t1 = f1.createVariable(\"time\",\"i\",(\"time\",))\n",
      "     |      >>> t2 = f2.createVariable(\"time\",\"i\",(\"time\",))\n",
      "     |      >>> t1.units = \"days since 2000-01-01\"\n",
      "     |      >>> t2.units = \"days since 2000-02-01\"\n",
      "     |      >>> t1.calendar = \"standard\"\n",
      "     |      >>> t2.calendar = \"standard\"\n",
      "     |      >>> t1[:] = numpy.arange(31)\n",
      "     |      >>> t2[:] = numpy.arange(30)\n",
      "     |      >>> f1.close()\n",
      "     |      >>> f2.close()\n",
      "     |      >>> # Read the two files in at once, in one Dataset.\n",
      "     |      >>> f = MFDataset(\"mftest*nc\")\n",
      "     |      >>> t = f.variables[\"time\"]\n",
      "     |      >>> print t.units\n",
      "     |      days since 2000-01-01\n",
      "     |      >>> print t[32] # The value written in the file, inconsistent with the MF time units.\n",
      "     |      1\n",
      "     |      >>> T = MFTime(t)\n",
      "     |      >>> print T[32]\n",
      "     |      32\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MFTime\n",
      "     |      _Variable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, elem)\n",
      "     |  \n",
      "     |  __init__(self, time, units=None, calendar=None)\n",
      "     |      **`__init__(self, time, units=None, calendar=None)`**\n",
      "     |      \n",
      "     |      Create a time Variable with units consistent across a multifile\n",
      "     |      dataset.\n",
      "     |      \n",
      "     |      **`time`**: Time variable from a `netCDF4.MFDataset`.\n",
      "     |      \n",
      "     |      **`units`**: Time units, for example, `'days since 1979-01-01'`. If `None`,\n",
      "     |      use the units from the master variable.\n",
      "     |      \n",
      "     |      **`calendar`**: Calendar overload to use across all files, for example,\n",
      "     |      `'standard'` or `'gregorian'`. If `None`, check that the calendar attribute\n",
      "     |      is present on each variable and values are unique across files raising a\n",
      "     |      `ValueError` otherwise.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _Variable:\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  ncattrs(self)\n",
      "     |  \n",
      "     |  set_always_mask(self, val)\n",
      "     |  \n",
      "     |  set_auto_chartostring(self, val)\n",
      "     |  \n",
      "     |  set_auto_mask(self, val)\n",
      "     |  \n",
      "     |  set_auto_maskandscale(self, val)\n",
      "     |  \n",
      "     |  set_auto_scale(self, val)\n",
      "     |  \n",
      "     |  typecode(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _Variable:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class VLType(builtins.object)\n",
      "     |  A `netCDF4.VLType` instance is used to describe a variable length (VLEN) data\n",
      "     |  type, and can be passed to the the `netCDF4.Dataset.createVariable` method of\n",
      "     |  a `netCDF4.Dataset` or `netCDF4.Group` instance. See\n",
      "     |  `netCDF4.VLType.__init__` for more details.\n",
      "     |  \n",
      "     |  The instance variables `dtype` and `name` should not be modified by\n",
      "     |  the user.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      **`__init__(group, datatype, datatype_name)`**\n",
      "     |      \n",
      "     |      VLType constructor.\n",
      "     |      \n",
      "     |      **`group`**: `netCDF4.Group` instance to associate with the VLEN datatype.\n",
      "     |      \n",
      "     |      **`datatype`**: An numpy dtype object describing the component type for the\n",
      "     |      variable length array.\n",
      "     |      \n",
      "     |      **`datatype_name`**: a Python string containing a description of the\n",
      "     |      VLEN data type.\n",
      "     |      \n",
      "     |      ***`Note`***: `netCDF4.VLType` instances should be created using the\n",
      "     |      `netCDF4.Dataset.createVLType`\n",
      "     |      method of a `netCDF4.Dataset` or `netCDF4.Group` instance, not using this class directly.\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  name\n",
      "    \n",
      "    class Variable(builtins.object)\n",
      "     |  A netCDF `netCDF4.Variable` is used to read and write netCDF data.  They are\n",
      "     |  analogous to numpy array objects. See `netCDF4.Variable.__init__` for more\n",
      "     |  details.\n",
      "     |  \n",
      "     |  A list of attribute names corresponding to netCDF attributes defined for\n",
      "     |  the variable can be obtained with the `netCDF4.Variable.ncattrs` method. These\n",
      "     |  attributes can be created by assigning to an attribute of the\n",
      "     |  `netCDF4.Variable` instance. A dictionary containing all the netCDF attribute\n",
      "     |  name/value pairs is provided by the `__dict__` attribute of a\n",
      "     |  `netCDF4.Variable` instance.\n",
      "     |  \n",
      "     |  The following class variables are read-only:\n",
      "     |  \n",
      "     |  **`dimensions`**: A tuple containing the names of the\n",
      "     |  dimensions associated with this variable.\n",
      "     |  \n",
      "     |  **`dtype`**: A numpy dtype object describing the\n",
      "     |  variable's data type.\n",
      "     |  \n",
      "     |  **`ndim`**: The number of variable dimensions.\n",
      "     |  \n",
      "     |  **`shape`**: A tuple with the current shape (length of all dimensions).\n",
      "     |  \n",
      "     |  **`scale`**: If True, `scale_factor` and `add_offset` are\n",
      "     |  applied, and signed integer data is automatically converted to\n",
      "     |  unsigned integer data if the `_Unsigned` attribute is set.\n",
      "     |  Default is `True`, can be reset using `netCDF4.Variable.set_auto_scale` and\n",
      "     |  `netCDF4.Variable.set_auto_maskandscale` methods.\n",
      "     |  \n",
      "     |  **`mask`**: If True, data is automatically converted to/from masked\n",
      "     |  arrays when missing values or fill values are present. Default is `True`, can be\n",
      "     |  reset using `netCDF4.Variable.set_auto_mask` and `netCDF4.Variable.set_auto_maskandscale`\n",
      "     |  methods.\n",
      "     |  \n",
      "     |  **`chartostring`**: If True, data is automatically converted to/from character\n",
      "     |  arrays to string arrays when the `_Encoding` variable attribute is set.\n",
      "     |  Default is `True`, can be reset using\n",
      "     |  `netCDF4.Variable.set_auto_chartostring` method.\n",
      "     |  \n",
      "     |  **`least_significant_digit`**: Describes the power of ten of the\n",
      "     |  smallest decimal place in the data the contains a reliable value.  Data is\n",
      "     |  truncated to this decimal place when it is assigned to the `netCDF4.Variable`\n",
      "     |  instance. If `None`, the data is not truncated.\n",
      "     |  \n",
      "     |  **`__orthogonal_indexing__`**: Always `True`.  Indicates to client code\n",
      "     |  that the object supports 'orthogonal indexing', which means that slices\n",
      "     |  that are 1d arrays or lists slice along each dimension independently.  This\n",
      "     |  behavior is similar to Fortran or Matlab, but different than numpy.\n",
      "     |  \n",
      "     |  **`datatype`**: numpy data type (for primitive data types) or VLType/CompoundType\n",
      "     |   instance (for compound or vlen data types).\n",
      "     |  \n",
      "     |  **`name`**: String name.\n",
      "     |  \n",
      "     |  **`size`**: The number of stored elements.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __array__(...)\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getattr__(...)\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(self, key, /)\n",
      "     |      Return self[key].\n",
      "     |  \n",
      "     |  __init__(...)\n",
      "     |      **`__init__(self, group, name, datatype, dimensions=(), zlib=False,\n",
      "     |      complevel=4, shuffle=True, fletcher32=False, contiguous=False,\n",
      "     |      chunksizes=None, endian='native',\n",
      "     |      least_significant_digit=None,fill_value=None)`**\n",
      "     |      \n",
      "     |      `netCDF4.Variable` constructor.\n",
      "     |      \n",
      "     |      **`group`**: `netCDF4.Group` or `netCDF4.Dataset` instance to associate with variable.\n",
      "     |      \n",
      "     |      **`name`**: Name of the variable.\n",
      "     |      \n",
      "     |      **`datatype`**: `netCDF4.Variable` data type. Can be specified by providing a\n",
      "     |      numpy dtype object, or a string that describes a numpy dtype object.\n",
      "     |      Supported values, corresponding to `str` attribute of numpy dtype\n",
      "     |      objects, include `'f4'` (32-bit floating point), `'f8'` (64-bit floating\n",
      "     |      point), `'i4'` (32-bit signed integer), `'i2'` (16-bit signed integer),\n",
      "     |      `'i8'` (64-bit signed integer), `'i4'` (8-bit signed integer), `'i1'`\n",
      "     |      (8-bit signed integer), `'u1'` (8-bit unsigned integer), `'u2'` (16-bit\n",
      "     |      unsigned integer), `'u4'` (32-bit unsigned integer), `'u8'` (64-bit\n",
      "     |      unsigned integer), or `'S1'` (single-character string).  From\n",
      "     |      compatibility with Scientific.IO.NetCDF, the old Numeric single character\n",
      "     |      typecodes can also be used (`'f'` instead of `'f4'`, `'d'` instead of\n",
      "     |      `'f8'`, `'h'` or `'s'` instead of `'i2'`, `'b'` or `'B'` instead of\n",
      "     |      `'i1'`, `'c'` instead of `'S1'`, and `'i'` or `'l'` instead of\n",
      "     |      `'i4'`). `datatype` can also be a `netCDF4.CompoundType` instance\n",
      "     |      (for a structured, or compound array), a `netCDF4.VLType` instance\n",
      "     |      (for a variable-length array), or the python `str` builtin\n",
      "     |      (for a variable-length string array). Numpy string and unicode datatypes with\n",
      "     |      length greater than one are aliases for `str`.\n",
      "     |      \n",
      "     |      **`dimensions`**: a tuple containing the variable's dimension names\n",
      "     |      (defined previously with `createDimension`). Default is an empty tuple\n",
      "     |      which means the variable is a scalar (and therefore has no dimensions).\n",
      "     |      \n",
      "     |      **`zlib`**: if `True`, data assigned to the `netCDF4.Variable`\n",
      "     |      instance is compressed on disk. Default `False`.\n",
      "     |      \n",
      "     |      **`complevel`**: the level of zlib compression to use (1 is the fastest,\n",
      "     |      but poorest compression, 9 is the slowest but best compression). Default 4.\n",
      "     |      Ignored if `zlib=False`.\n",
      "     |      \n",
      "     |      **`shuffle`**: if `True`, the HDF5 shuffle filter is applied\n",
      "     |      to improve compression. Default `True`. Ignored if `zlib=False`.\n",
      "     |      \n",
      "     |      **`fletcher32`**: if `True` (default `False`), the Fletcher32 checksum\n",
      "     |      algorithm is used for error detection.\n",
      "     |      \n",
      "     |      **`contiguous`**: if `True` (default `False`), the variable data is\n",
      "     |      stored contiguously on disk.  Default `False`. Setting to `True` for\n",
      "     |      a variable with an unlimited dimension will trigger an error.\n",
      "     |      \n",
      "     |      **`chunksizes`**: Can be used to specify the HDF5 chunksizes for each\n",
      "     |      dimension of the variable. A detailed discussion of HDF chunking and I/O\n",
      "     |      performance is available\n",
      "     |      [here](http://www.hdfgroup.org/HDF5/doc/H5.user/Chunking.html).\n",
      "     |      Basically, you want the chunk size for each dimension to match as\n",
      "     |      closely as possible the size of the data block that users will read\n",
      "     |      from the file. `chunksizes` cannot be set if `contiguous=True`.\n",
      "     |      \n",
      "     |      **`endian`**: Can be used to control whether the\n",
      "     |      data is stored in little or big endian format on disk. Possible\n",
      "     |      values are `little, big` or `native` (default). The library\n",
      "     |      will automatically handle endian conversions when the data is read,\n",
      "     |      but if the data is always going to be read on a computer with the\n",
      "     |      opposite format as the one used to create the file, there may be\n",
      "     |      some performance advantage to be gained by setting the endian-ness.\n",
      "     |      For netCDF 3 files (that don't use HDF5), only `endian='native'` is allowed.\n",
      "     |      \n",
      "     |      The `zlib, complevel, shuffle, fletcher32, contiguous` and `chunksizes`\n",
      "     |      keywords are silently ignored for netCDF 3 files that do not use HDF5.\n",
      "     |      \n",
      "     |      **`least_significant_digit`**: If specified, variable data will be\n",
      "     |      truncated (quantized). In conjunction with `zlib=True` this produces\n",
      "     |      'lossy', but significantly more efficient compression. For example, if\n",
      "     |      `least_significant_digit=1`, data will be quantized using\n",
      "     |      around(scale*data)/scale, where scale = 2**bits, and bits is determined\n",
      "     |      so that a precision of 0.1 is retained (in this case bits=4). Default is\n",
      "     |      `None`, or no quantization.\n",
      "     |      \n",
      "     |      **`fill_value`**:  If specified, the default netCDF `_FillValue` (the\n",
      "     |      value that the variable gets filled with before any data is written to it)\n",
      "     |      is replaced with this value.  If fill_value is set to `False`, then\n",
      "     |      the variable is not pre-filled. The default netCDF fill values can be found\n",
      "     |      in `netCDF4.default_fillvals`.\n",
      "     |      \n",
      "     |      ***Note***: `netCDF4.Variable` instances should be created using the\n",
      "     |      `netCDF4.Dataset.createVariable` method of a `netCDF4.Dataset` or\n",
      "     |      `netCDF4.Group` instance, not using this class directly.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __unicode__(...)\n",
      "     |  \n",
      "     |  assignValue(...)\n",
      "     |      **`assignValue(self, val)`**\n",
      "     |      \n",
      "     |      assign a value to a scalar variable.  Provided for compatibility with\n",
      "     |      Scientific.IO.NetCDF, can also be done by assigning to an Ellipsis slice ([...]).\n",
      "     |  \n",
      "     |  chunking(...)\n",
      "     |      **`chunking(self)`**\n",
      "     |      \n",
      "     |      return variable chunking information.  If the dataset is\n",
      "     |      defined to be contiguous (and hence there is no chunking) the word 'contiguous'\n",
      "     |      is returned.  Otherwise, a sequence with the chunksize for\n",
      "     |      each dimension is returned.\n",
      "     |  \n",
      "     |  delncattr(...)\n",
      "     |      **`delncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      delete a netCDF variable attribute.  Use if you need to delete a\n",
      "     |      netCDF attribute with the same name as one of the reserved python\n",
      "     |      attributes.\n",
      "     |  \n",
      "     |  endian(...)\n",
      "     |      **`endian(self)`**\n",
      "     |      \n",
      "     |      return endian-ness (`little,big,native`) of variable (as stored in HDF5 file).\n",
      "     |  \n",
      "     |  filters(...)\n",
      "     |      **`filters(self)`**\n",
      "     |      \n",
      "     |      return dictionary containing HDF5 filter parameters.\n",
      "     |  \n",
      "     |  getValue(...)\n",
      "     |      **`getValue(self)`**\n",
      "     |      \n",
      "     |      get the value of a scalar variable.  Provided for compatibility with\n",
      "     |      Scientific.IO.NetCDF, can also be done by slicing with an Ellipsis ([...]).\n",
      "     |  \n",
      "     |  get_dims(...)\n",
      "     |      **`get_dims(self)`**\n",
      "     |      \n",
      "     |      return a tuple of `netCDF4.Dimension` instances associated with this\n",
      "     |      `netCDF4.Variable.\n",
      "     |  \n",
      "     |  get_var_chunk_cache(...)\n",
      "     |      **`get_var_chunk_cache(self)`**\n",
      "     |      \n",
      "     |      return variable chunk cache information in a tuple (size,nelems,preemption).\n",
      "     |      See netcdf C library documentation for `nc_get_var_chunk_cache` for\n",
      "     |      details.\n",
      "     |  \n",
      "     |  getncattr(...)\n",
      "     |      **`getncattr(self,name)`**\n",
      "     |      \n",
      "     |      retrieve a netCDF variable attribute.  Use if you need to set a\n",
      "     |      netCDF attribute with the same name as one of the reserved python\n",
      "     |      attributes.\n",
      "     |      \n",
      "     |      option kwarg `encoding` can be used to specify the\n",
      "     |      character encoding of a string attribute (default is `utf-8`).\n",
      "     |  \n",
      "     |  group(...)\n",
      "     |      **`group(self)`**\n",
      "     |      \n",
      "     |      return the group that this `netCDF4.Variable` is a member of.\n",
      "     |  \n",
      "     |  ncattrs(...)\n",
      "     |      **`ncattrs(self)`**\n",
      "     |      \n",
      "     |      return netCDF attribute names for this `netCDF4.Variable` in a list.\n",
      "     |  \n",
      "     |  renameAttribute(...)\n",
      "     |      **`renameAttribute(self, oldname, newname)`**\n",
      "     |      \n",
      "     |      rename a `netCDF4.Variable` attribute named `oldname` to `newname`.\n",
      "     |  \n",
      "     |  set_always_mask(...)\n",
      "     |      **`set_always_mask(self,always_mask)`**\n",
      "     |      \n",
      "     |      turn on or off conversion of data without missing values to regular\n",
      "     |      numpy arrays.\n",
      "     |      \n",
      "     |      If `always_mask` is set to `True` then a masked array with no missing\n",
      "     |      values is converted to a regular numpy array.\n",
      "     |      \n",
      "     |      The default value of `always_mask` is `True` (conversions to regular\n",
      "     |      numpy arrays are not performed).\n",
      "     |  \n",
      "     |  set_auto_chartostring(...)\n",
      "     |      **`set_auto_chartostring(self,chartostring)`**\n",
      "     |      \n",
      "     |      turn on or off automatic conversion of character variable data to and\n",
      "     |      from numpy fixed length string arrays when the `_Encoding` variable attribute\n",
      "     |      is set.\n",
      "     |      \n",
      "     |      If `chartostring` is set to `True`, when data is read from a character variable\n",
      "     |      (dtype = `S1`) that has an `_Encoding` attribute, it is converted to a numpy\n",
      "     |      fixed length unicode string array (dtype = `UN`, where `N` is the length\n",
      "     |      of the the rightmost dimension of the variable).  The value of `_Encoding`\n",
      "     |      is the unicode encoding that is used to decode the bytes into strings.\n",
      "     |      \n",
      "     |      When numpy string data is written to a variable it is converted back to\n",
      "     |      indiviual bytes, with the number of bytes in each string equalling the\n",
      "     |      rightmost dimension of the variable.\n",
      "     |      \n",
      "     |      The default value of `chartostring` is `True`\n",
      "     |      (automatic conversions are performed).\n",
      "     |  \n",
      "     |  set_auto_mask(...)\n",
      "     |      **`set_auto_mask(self,mask)`**\n",
      "     |      \n",
      "     |      turn on or off automatic conversion of variable data to and\n",
      "     |      from masked arrays .\n",
      "     |      \n",
      "     |      If `mask` is set to `True`, when data is read from a variable\n",
      "     |      it is converted to a masked array if any of the values are exactly\n",
      "     |      equal to the either the netCDF _FillValue or the value specified by the\n",
      "     |      missing_value variable attribute. The fill_value of the masked array\n",
      "     |      is set to the missing_value attribute (if it exists), otherwise\n",
      "     |      the netCDF _FillValue attribute (which has a default value\n",
      "     |      for each data type).  When data is written to a variable, the masked\n",
      "     |      array is converted back to a regular numpy array by replacing all the\n",
      "     |      masked values by the missing_value attribute of the variable (if it\n",
      "     |      exists).  If the variable has no missing_value attribute, the _FillValue\n",
      "     |      is used instead. If the variable has valid_min/valid_max and\n",
      "     |      missing_value attributes, data outside the specified range will be\n",
      "     |      set to missing_value.\n",
      "     |      \n",
      "     |      The default value of `mask` is `True`\n",
      "     |      (automatic conversions are performed).\n",
      "     |  \n",
      "     |  set_auto_maskandscale(...)\n",
      "     |      **`set_auto_maskandscale(self,maskandscale)`**\n",
      "     |      \n",
      "     |      turn on or off automatic conversion of variable data to and\n",
      "     |      from masked arrays, automatic packing/unpacking of variable\n",
      "     |      data using `scale_factor` and `add_offset` attributes and\n",
      "     |      automatic conversion of signed integer data to unsigned integer\n",
      "     |      data if the `_Unsigned` attribute exists.\n",
      "     |      \n",
      "     |      If `maskandscale` is set to `True`, when data is read from a variable\n",
      "     |      it is converted to a masked array if any of the values are exactly\n",
      "     |      equal to the either the netCDF _FillValue or the value specified by the\n",
      "     |      missing_value variable attribute. The fill_value of the masked array\n",
      "     |      is set to the missing_value attribute (if it exists), otherwise\n",
      "     |      the netCDF _FillValue attribute (which has a default value\n",
      "     |      for each data type).  When data is written to a variable, the masked\n",
      "     |      array is converted back to a regular numpy array by replacing all the\n",
      "     |      masked values by the missing_value attribute of the variable (if it\n",
      "     |      exists).  If the variable has no missing_value attribute, the _FillValue\n",
      "     |      is used instead. If the variable has valid_min/valid_max and\n",
      "     |      missing_value attributes, data outside the specified range will be\n",
      "     |      set to missing_value.\n",
      "     |      \n",
      "     |      If `maskandscale` is set to `True`, and the variable has a\n",
      "     |      `scale_factor` or an `add_offset` attribute, then data read\n",
      "     |      from that variable is unpacked using::\n",
      "     |      \n",
      "     |          data = self.scale_factor*data + self.add_offset\n",
      "     |      \n",
      "     |      When data is written to a variable it is packed using::\n",
      "     |      \n",
      "     |          data = (data - self.add_offset)/self.scale_factor\n",
      "     |      \n",
      "     |      If either scale_factor is present, but add_offset is missing, add_offset\n",
      "     |      is assumed zero.  If add_offset is present, but scale_factor is missing,\n",
      "     |      scale_factor is assumed to be one.\n",
      "     |      For more information on how `scale_factor` and `add_offset` can be\n",
      "     |      used to provide simple compression, see the\n",
      "     |      [PSD metadata conventions](http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml).\n",
      "     |      \n",
      "     |      In addition, if `maskandscale` is set to `True`, and if the variable has an\n",
      "     |      attribute `_Unsigned` set, and the variable has a signed integer data type,\n",
      "     |      a view to the data is returned with the corresponding unsigned integer data type.\n",
      "     |      This convention is used by the netcdf-java library to save unsigned integer\n",
      "     |      data in `NETCDF3` or `NETCDF4_CLASSIC` files (since the `NETCDF3`\n",
      "     |      data model does not have unsigned integer data types).\n",
      "     |      \n",
      "     |      The default value of `maskandscale` is `True`\n",
      "     |      (automatic conversions are performed).\n",
      "     |  \n",
      "     |  set_auto_scale(...)\n",
      "     |      **`set_auto_scale(self,scale)`**\n",
      "     |      \n",
      "     |      turn on or off automatic packing/unpacking of variable\n",
      "     |      data using `scale_factor` and `add_offset` attributes.\n",
      "     |      Also turns on and off automatic conversion of signed integer data\n",
      "     |      to unsigned integer data if the variable has an `_Unsigned`\n",
      "     |      attribute.\n",
      "     |      \n",
      "     |      If `scale` is set to `True`, and the variable has a\n",
      "     |      `scale_factor` or an `add_offset` attribute, then data read\n",
      "     |      from that variable is unpacked using::\n",
      "     |      \n",
      "     |          data = self.scale_factor*data + self.add_offset\n",
      "     |      \n",
      "     |      When data is written to a variable it is packed using::\n",
      "     |      \n",
      "     |          data = (data - self.add_offset)/self.scale_factor\n",
      "     |      \n",
      "     |      If either scale_factor is present, but add_offset is missing, add_offset\n",
      "     |      is assumed zero.  If add_offset is present, but scale_factor is missing,\n",
      "     |      scale_factor is assumed to be one.\n",
      "     |      For more information on how `scale_factor` and `add_offset` can be\n",
      "     |      used to provide simple compression, see the\n",
      "     |      [PSD metadata conventions](http://www.esrl.noaa.gov/psd/data/gridded/conventions/cdc_netcdf_standard.shtml).\n",
      "     |      \n",
      "     |      In addition, if `scale` is set to `True`, and if the variable has an\n",
      "     |      attribute `_Unsigned` set, and the variable has a signed integer data type,\n",
      "     |      a view to the data is returned with the corresponding unsigned integer datatype.\n",
      "     |      This convention is used by the netcdf-java library to save unsigned integer\n",
      "     |      data in `NETCDF3` or `NETCDF4_CLASSIC` files (since the `NETCDF3`\n",
      "     |      data model does not have unsigned integer data types).\n",
      "     |      \n",
      "     |      The default value of `scale` is `True`\n",
      "     |      (automatic conversions are performed).\n",
      "     |  \n",
      "     |  set_collective(...)\n",
      "     |      **`set_collective(self,True_or_False)`**\n",
      "     |      \n",
      "     |      turn on or off collective parallel IO access. Ignored if file is not\n",
      "     |      open for parallel access.\n",
      "     |  \n",
      "     |  set_ncstring_attrs(...)\n",
      "     |      **`set_always_mask(self,ncstring_attrs)`**\n",
      "     |      \n",
      "     |      turn on or off creating NC_STRING string attributes.\n",
      "     |      \n",
      "     |      If `ncstring_attrs` is set to `True` then text attributes will be variable-length\n",
      "     |      NC_STRINGs.\n",
      "     |      \n",
      "     |      The default value of `ncstring_attrs` is `False` (writing ascii text attributes as\n",
      "     |      NC_CHAR).\n",
      "     |  \n",
      "     |  set_var_chunk_cache(...)\n",
      "     |      **`set_var_chunk_cache(self,size=None,nelems=None,preemption=None)`**\n",
      "     |      \n",
      "     |      change variable chunk cache settings.\n",
      "     |      See netcdf C library documentation for `nc_set_var_chunk_cache` for\n",
      "     |      details.\n",
      "     |  \n",
      "     |  setncattr(...)\n",
      "     |      **`setncattr(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF variable attribute using name,value pair.  Use if you need to set a\n",
      "     |      netCDF attribute with the same name as one of the reserved python\n",
      "     |      attributes.\n",
      "     |  \n",
      "     |  setncattr_string(...)\n",
      "     |      **`setncattr_string(self,name,value)`**\n",
      "     |      \n",
      "     |      set a netCDF variable string attribute using name,value pair.\n",
      "     |      Use if you need to ensure that a netCDF attribute is created with type\n",
      "     |      `NC_STRING` if the file format is `NETCDF4`.\n",
      "     |      Use if you need to set an attribute to an array of variable-length strings.\n",
      "     |  \n",
      "     |  setncatts(...)\n",
      "     |      **`setncatts(self,attdict)`**\n",
      "     |      \n",
      "     |      set a bunch of netCDF variable attributes at once using a python dictionary.\n",
      "     |      This may be faster when setting a lot of attributes for a `NETCDF3`\n",
      "     |      formatted file, since nc_redef/nc_enddef is not called in between setting\n",
      "     |      each attribute\n",
      "     |  \n",
      "     |  use_nc_get_vars(...)\n",
      "     |      **`use_nc_get_vars(self,_use_get_vars)`**\n",
      "     |      \n",
      "     |      enable the use of netcdf library routine `nc_get_vars`\n",
      "     |      to retrieve strided variable slices.  By default,\n",
      "     |      `nc_get_vars` may not used by default (depending on the\n",
      "     |      version of the netcdf-c library being used) since it may be\n",
      "     |      slower than multiple calls to the unstrided read routine `nc_get_vara`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __orthogonal_indexing__\n",
      "     |  \n",
      "     |  always_mask\n",
      "     |  \n",
      "     |  chartostring\n",
      "     |  \n",
      "     |  datatype\n",
      "     |      numpy data type (for primitive data types) or\n",
      "     |      VLType/CompoundType/EnumType instance \n",
      "     |      (for compound, vlen  or enum data types)\n",
      "     |  \n",
      "     |  dimensions\n",
      "     |      get variables's dimension names\n",
      "     |  \n",
      "     |  dtype\n",
      "     |  \n",
      "     |  mask\n",
      "     |  \n",
      "     |  name\n",
      "     |      string name of Variable instance\n",
      "     |  \n",
      "     |  ndim\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  shape\n",
      "     |      find current sizes of all variable dimensions\n",
      "     |  \n",
      "     |  size\n",
      "     |      Return the number of stored elements.\n",
      "\n",
      "FUNCTIONS\n",
      "    chartostring(...)\n",
      "        **`chartostring(b,encoding='utf-8')`**\n",
      "        \n",
      "        convert a character array to a string array with one less dimension.\n",
      "        \n",
      "        **`b`**:  Input character array (numpy datatype `'S1'` or `'U1'`).\n",
      "        Will be converted to a array of strings, where each string has a fixed\n",
      "        length of `b.shape[-1]` characters.\n",
      "        \n",
      "        optional kwarg `encoding` can be used to specify character encoding (default\n",
      "        `utf-8`). If `encoding` is 'none' or 'bytes', a `numpy.string_` btye array is\n",
      "        returned.\n",
      "        \n",
      "        returns a numpy string array with datatype `'UN'` (or `'SN'`) and shape\n",
      "        `b.shape[:-1]` where where `N=b.shape[-1]`.\n",
      "    \n",
      "    date2index(...)\n",
      "        date2index(dates, nctime, calendar=None, select='exact')\n",
      "        \n",
      "        Return indices of a netCDF time variable corresponding to the given dates.\n",
      "        \n",
      "        **`dates`**: A datetime object or a sequence of datetime objects.\n",
      "        The datetime objects should not include a time-zone offset.\n",
      "        \n",
      "        **`nctime`**: A netCDF time variable object. The nctime object must have a\n",
      "        `units` attribute.\n",
      "        \n",
      "        **`calendar`**: describes the calendar used in the time calculations.\n",
      "        All the values currently defined in the\n",
      "        [CF metadata convention](http://cfconventions.org)\n",
      "        Valid calendars `'standard', 'gregorian', 'proleptic_gregorian'\n",
      "        'noleap', '365_day', '360_day', 'julian', 'all_leap', '366_day'`.\n",
      "        Default is `'standard'`, which is a mixed Julian/Gregorian calendar.\n",
      "        If `calendar` is None, its value is given by `nctime.calendar` or\n",
      "        `standard` if no such attribute exists.\n",
      "        \n",
      "        **`select`**: `'exact', 'before', 'after', 'nearest'`\n",
      "        The index selection method. `exact` will return the indices perfectly\n",
      "        matching the dates given. `before` and `after` will return the indices\n",
      "        corresponding to the dates just before or just after the given dates if\n",
      "        an exact match cannot be found. `nearest` will return the indices that\n",
      "        correspond to the closest dates.\n",
      "        \n",
      "        returns an index (indices) of the netCDF time variable corresponding\n",
      "        to the given datetime object(s).\n",
      "    \n",
      "    date2num(...)\n",
      "        date2num(dates,units,calendar='standard')\n",
      "        \n",
      "        Return numeric time values given datetime objects. The units\n",
      "        of the numeric time values are described by the `units` argument\n",
      "        and the `calendar` keyword. The datetime objects must\n",
      "        be in UTC with no time-zone offset.  If there is a\n",
      "        time-zone offset in `units`, it will be applied to the\n",
      "        returned numeric values.\n",
      "        \n",
      "        **`dates`**: A datetime object or a sequence of datetime objects.\n",
      "        The datetime objects should not include a time-zone offset.\n",
      "        \n",
      "        **`units`**: a string of the form `<time units> since <reference time>`\n",
      "        describing the time units. `<time units>` can be days, hours, minutes,\n",
      "        seconds, milliseconds or microseconds. `<reference time>` is the time\n",
      "        origin. `months_since` is allowed *only* for the `360_day` calendar.\n",
      "        \n",
      "        **`calendar`**: describes the calendar used in the time calculations.\n",
      "        All the values currently defined in the\n",
      "        [CF metadata convention](http://cfconventions.org)\n",
      "        Valid calendars `'standard', 'gregorian', 'proleptic_gregorian'\n",
      "        'noleap', '365_day', '360_day', 'julian', 'all_leap', '366_day'`.\n",
      "        Default is `'standard'`, which is a mixed Julian/Gregorian calendar.\n",
      "        \n",
      "        returns a numeric time value, or an array of numeric time values\n",
      "        with approximately 100 microsecond accuracy.\n",
      "    \n",
      "    getlibversion(...)\n",
      "        **`getlibversion()`**\n",
      "        \n",
      "        returns a string describing the version of the netcdf library\n",
      "        used to build the module, and when it was built.\n",
      "    \n",
      "    num2date(...)\n",
      "        num2date(times,units,calendar='standard')\n",
      "        \n",
      "        Return datetime objects given numeric time values. The units\n",
      "        of the numeric time values are described by the `units` argument\n",
      "        and the `calendar` keyword. The returned datetime objects represent\n",
      "        UTC with no time-zone offset, even if the specified\n",
      "        `units` contain a time-zone offset.\n",
      "        \n",
      "        **`times`**: numeric time values.\n",
      "        \n",
      "        **`units`**: a string of the form `<time units> since <reference time>`\n",
      "        describing the time units. `<time units>` can be days, hours, minutes,\n",
      "        seconds, milliseconds or microseconds. `<reference time>` is the time\n",
      "        origin. `months_since` is allowed *only* for the `360_day` calendar.\n",
      "        \n",
      "        **`calendar`**: describes the calendar used in the time calculations.\n",
      "        All the values currently defined in the\n",
      "        [CF metadata convention](http://cfconventions.org)\n",
      "        Valid calendars `'standard', 'gregorian', 'proleptic_gregorian'\n",
      "        'noleap', '365_day', '360_day', 'julian', 'all_leap', '366_day'`.\n",
      "        Default is `'standard'`, which is a mixed Julian/Gregorian calendar.\n",
      "        \n",
      "        **`only_use_cftime_datetimes`**: if False (default), datetime.datetime\n",
      "        objects are returned from num2date where possible; if True dates which\n",
      "        subclass cftime.datetime are returned for all calendars.\n",
      "        \n",
      "        returns a datetime instance, or an array of datetime instances with\n",
      "        approximately 100 microsecond accuracy.\n",
      "        \n",
      "        ***Note***: The datetime instances returned are 'real' python datetime\n",
      "        objects if `calendar='proleptic_gregorian'`, or\n",
      "        `calendar='standard'` or `'gregorian'`\n",
      "        and the date is after the breakpoint between the Julian and\n",
      "        Gregorian calendars (1582-10-15). Otherwise, they are 'phony' datetime\n",
      "        objects which support some but not all the methods of 'real' python\n",
      "        datetime objects. The datetime instances\n",
      "        do not contain a time-zone offset, even if the specified `units`\n",
      "        contains one.\n",
      "    \n",
      "    stringtoarr(...)\n",
      "        **`stringtoarr(a, NUMCHARS,dtype='S')`**\n",
      "        \n",
      "        convert a string to a character array of length `NUMCHARS`\n",
      "        \n",
      "        **`a`**:  Input python string.\n",
      "        \n",
      "        **`NUMCHARS`**:  number of characters used to represent string\n",
      "        (if len(a) < `NUMCHARS`, it will be padded on the right with blanks).\n",
      "        \n",
      "        **`dtype`**:  type of numpy array to return.  Default is `'S'`, which\n",
      "        means an array of dtype `'S1'` will be returned.  If dtype=`'U'`, a\n",
      "        unicode array (dtype = `'U1'`) will be returned.\n",
      "        \n",
      "        returns a rank 1 numpy character array of length NUMCHARS with datatype `'S1'`\n",
      "        (default) or `'U1'` (if dtype=`'U'`)\n",
      "    \n",
      "    stringtochar(...)\n",
      "        **`stringtochar(a,encoding='utf-8')`**\n",
      "        \n",
      "        convert a string array to a character array with one extra dimension\n",
      "        \n",
      "        **`a`**:  Input numpy string array with numpy datatype `'SN'` or `'UN'`, where N\n",
      "        is the number of characters in each string.  Will be converted to\n",
      "        an array of characters (datatype `'S1'` or `'U1'`) of shape `a.shape + (N,)`.\n",
      "        \n",
      "        optional kwarg `encoding` can be used to specify character encoding (default\n",
      "        `utf-8`). If `encoding` is 'none' or 'bytes', a `numpy.string_` the input array\n",
      "        is treated a raw byte strings (`numpy.string_`).\n",
      "        \n",
      "        returns a numpy character array with datatype `'S1'` or `'U1'`\n",
      "        and shape `a.shape + (N,)`, where N is the length of each string in a.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Dataset', 'Variable', 'Dimension', 'Group', 'MFDataset', '...\n",
      "    __has_cdf5_format__ = 1\n",
      "    __has_nc_create_mem__ = 1\n",
      "    __has_nc_inq_format_extended__ = 1\n",
      "    __has_nc_inq_path__ = 1\n",
      "    __has_nc_open_mem__ = 1\n",
      "    __has_parallel4_support__ = 0\n",
      "    __has_pnetcdf_support__ = 0\n",
      "    __has_rename_grp__ = 1\n",
      "    __hdf5libversion__ = '1.10.4'\n",
      "    __netcdf4libversion__ = '4.6.2'\n",
      "    __pdoc__ = {'CompoundType.dtype': 'A numpy dtype object describing the...\n",
      "\n",
      "VERSION\n",
      "    1.5.1.2\n",
      "\n",
      "FILE\n",
      "    /home/mkelly/miniconda3/envs/analysis/lib/python3.7/site-packages/netCDF4/__init__.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
